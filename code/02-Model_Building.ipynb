{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dat[dat['response'].duplicated()] # The one duplicated response is the duplicated one from before\n",
    "#re.sub(r\"\\/*u\\/[\\S]+\", 'they', TEXT) # This works to replace usernames with they\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "\n",
    "rs = 91923 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/data_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['response']\n",
    "y = df['fake']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = rs, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.500062\n",
       "0    0.499938\n",
       "Name: fake, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, our baseline is 50% since half our data came from AI responses and half was real data that was gathered from reddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('densify', FunctionTransformer(lambda x: x.toarray(), accept_sparse = True)),\n",
    "        ('mnb', MultinomialNB())\n",
    "    ]\n",
    ")\n",
    "\n",
    "bnb_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('densify', FunctionTransformer(lambda x: x.toarray(), accept_sparse = True)),\n",
    "        ('bnb', BernoulliNB())\n",
    "    ]\n",
    ")\n",
    "\n",
    "gnb_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('densify', FunctionTransformer(lambda x: x.toarray(), accept_sparse = True)),\n",
    "        ('gnb', GaussianNB())\n",
    "    ]\n",
    ")\n",
    "\n",
    "logr_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('logr', LogisticRegression(max_iter = 500, n_jobs = -1))\n",
    "    ]\n",
    "    )\n",
    "\n",
    "rf_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('rf', RandomForestClassifier(random_state = rs, n_jobs = -1))\n",
    "    ])\n",
    "\n",
    "et_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('et', ExtraTreesClassifier(random_state = rs, n_jobs = -1))\n",
    "    ]\n",
    ")\n",
    "\n",
    "linear_svc_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('lsvc', SVC(kernel = 'linear'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "rsvc_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('rsvc', SVC(kernel = 'rbf'))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB Train: 0.8555240793201133\n",
      "MNB Test: 0.7661618027336535\n",
      "----\n",
      "BNB Train: 0.6961448454243133\n",
      "BNB Test: 0.6734392316217215\n",
      "----\n",
      "GNB Train: 0.9052839019583693\n",
      "GNB Test: 0.5755448836350203\n",
      "----\n",
      "LogReg Train: 0.9959354600320236\n",
      "LogReg Test: 0.879571481344662\n",
      "----\n",
      "RF Train: 1.0\n",
      "RF Test: 0.8762467676394533\n",
      "----\n",
      "ET Train: 1.0\n",
      "ET Test: 0.8943479867011451\n",
      "----\n",
      "LSVC Train: 0.9991378248552777\n",
      "LSVC Test: 0.8507572958995198\n",
      "----\n",
      "RSVC Train: 0.906269244980909\n",
      "RSVC Test: 0.8799408939785741\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "mnb_pipe.fit(X_train, y_train)\n",
    "bnb_pipe.fit(X_train, y_train)\n",
    "gnb_pipe.fit(X_train, y_train)\n",
    "logr_pipe.fit(X_train, y_train)\n",
    "rf_pipe.fit(X_train, y_train)\n",
    "et_pipe.fit(X_train, y_train)\n",
    "linear_svc_pipe.fit(X_train, y_train)\n",
    "rsvc_pipe.fit(X_train, y_train)\n",
    "\n",
    "print(\"MNB Train: {}\\nMNB Test: {}\\n----\".format(mnb_pipe.score(X_train, y_train), mnb_pipe.score(X_test, y_test)))\n",
    "print(\"BNB Train: {}\\nBNB Test: {}\\n----\".format(bnb_pipe.score(X_train, y_train), bnb_pipe.score(X_test, y_test)))\n",
    "print(\"GNB Train: {}\\nGNB Test: {}\\n----\".format(gnb_pipe.score(X_train, y_train), gnb_pipe.score(X_test, y_test)))\n",
    "print(\"LogReg Train: {}\\nLogReg Test: {}\\n----\".format(logr_pipe.score(X_train, y_train), logr_pipe.score(X_test, y_test)))\n",
    "print(\"RF Train: {}\\nRF Test: {}\\n----\".format(rf_pipe.score(X_train, y_train), rf_pipe.score(X_test, y_test)))\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_pipe.score(X_train, y_train), et_pipe.score(X_test, y_test)))\n",
    "print(\"LSVC Train: {}\\nLSVC Test: {}\\n----\".format(linear_svc_pipe.score(X_train, y_train), linear_svc_pipe.score(X_test, y_test)))\n",
    "print(\"RSVC Train: {}\\nRSVC Test: {}\\n----\".format(rsvc_pipe.score(X_train, y_train), rsvc_pipe.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With no attempt at tuning or cleaning the data structurally, Multiomial Naive Bayes has the best test performance but is moderately overfit while Bernoulli Naive Bayes performs worse by a lot but is barely overfit. Gaussian Naive Bayes defies all expectations and manages to perform worse than the other two methods on the test data while being the most overfit. Generally, this points towards Multinomial Naive Bayes being the best model of the Naive Bayes approaches but I'm not ready to commit to that just yet in case I can get the Bernoulli model to be somewhat competitive. Logistic Regression performs quite well, beating Multinomil Naive Bayes by 11% on the test set, however we do see that it is very overfit again with nearly a perfect score on our training set. With proper regularization and other hyperparameter tuning Logisitic Regression could end up being one of the best models for detecting AI responses but that will be investigated later on. Our ensemble classifiers perform similarly to Logistic Regression with Extra Trees beating it slightly. Both have perfect scores on the training data so tuning will be needed to address the overfit issue. Linear SVC has good results but doesn't match our higher test rates while still having an overfit issue however radial SVC is just a bit short of our ensemble methods but has much lower overfit compared to the other models. Radial SVC is a model that will definitely warrant further attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting deeper into modelling I want to try cleaning our responses slightly. There are two things I noted in our data: Reddit users tend to use two newline characters since reddit formatting requires it while the AI responses typically don't and real Reddit comments will sometimes reference other users by name in the format '/u/\\<name here\\>' or 'u/\\<name here\\>'. I plan to replace the former with a single newline character and the latter with 'they' instead and then I will see if that makes it a bit harder to predict which responses are fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Double newline count: 2961 \n",
      "A lot of observations have this double newline in them\n",
      "AI double newline count: 218\n",
      "Real double newline count: 2743\n"
     ]
    }
   ],
   "source": [
    "print('Double newline count: {} \\nA lot of observations have this double newline in them'.format(df[df['response'].str.contains('\\n\\n')].shape[0]))\n",
    "print('AI double newline count: {}'.format(df[(df['response'].str.contains('\\n\\n')) & (df['fake'] == 1)].shape[0])) # Not many AI models have the double newline\n",
    "print('Real double newline count: {}'.format(df[(df['response'].str.contains('\\n\\n')) & (df['fake'] == 0)].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of our double newlines are in the real comments so we'll proceed with removing them from our responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(165, 4)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['response'].str.contains('\\n\\n\\n')].shape # For some reason there are responses that use triple newline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['response_cleaned'] = df['response'].apply(lambda x: re.sub('[\\\\n]{2,}', '\\n', x))\n",
    "df[df['response_cleaned'].str.contains('\\n\\n')].shape # All gone now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['response_cleaned'] = df['response_cleaned'].apply(lambda x: re.sub(r\"\\/*u\\/[\\S]+\", 'they', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>response</th>\n",
       "      <th>fake</th>\n",
       "      <th>response_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [subreddit, response, fake, response_cleaned]\n",
       "Index: []"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['response_cleaned'].str.contains('u/')] # All usernames gone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that out of the way, let's see if our performance or overfitting has changed on any of our models from earlier. The assumption is that the newlines were relevant so our predictive power should have taken some hit but overfitting may not be effected since these problems should have affected both training and test sets evenly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['response_cleaned']\n",
    "y = df['fake']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = rs, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB Train: 0.8557704150757482\n",
      "MNB Test: 0.7669006280014776\n",
      "----\n",
      "BNB Train: 0.6963911811799482\n",
      "BNB Test: 0.6738086442556336\n",
      "----\n",
      "GNB Train: 0.9051607340805518\n",
      "GNB Test: 0.5755448836350203\n",
      "----\n",
      "LogReg Train: 0.9959354600320236\n",
      "LogReg Test: 0.8792020687107499\n",
      "----\n",
      "RF Train: 1.0\n",
      "RF Test: 0.8825267824159586\n",
      "----\n",
      "ET Train: 1.0\n",
      "ET Test: 0.8980421130402659\n",
      "----\n",
      "LSVC Train: 0.9991378248552777\n",
      "LSVC Test: 0.8507572958995198\n",
      "----\n",
      "RSVC Train: 0.9065155807365439\n",
      "RSVC Test: 0.8792020687107499\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "mnb_pipe.fit(X_train, y_train)\n",
    "bnb_pipe.fit(X_train, y_train)\n",
    "gnb_pipe.fit(X_train, y_train)\n",
    "logr_pipe.fit(X_train, y_train)\n",
    "rf_pipe.fit(X_train, y_train)\n",
    "et_pipe.fit(X_train, y_train)\n",
    "linear_svc_pipe.fit(X_train, y_train)\n",
    "rsvc_pipe.fit(X_train, y_train)\n",
    "\n",
    "print(\"MNB Train: {}\\nMNB Test: {}\\n----\".format(mnb_pipe.score(X_train, y_train), mnb_pipe.score(X_test, y_test)))\n",
    "print(\"BNB Train: {}\\nBNB Test: {}\\n----\".format(bnb_pipe.score(X_train, y_train), bnb_pipe.score(X_test, y_test)))\n",
    "print(\"GNB Train: {}\\nGNB Test: {}\\n----\".format(gnb_pipe.score(X_train, y_train), gnb_pipe.score(X_test, y_test)))\n",
    "print(\"LogReg Train: {}\\nLogReg Test: {}\\n----\".format(logr_pipe.score(X_train, y_train), logr_pipe.score(X_test, y_test)))\n",
    "print(\"RF Train: {}\\nRF Test: {}\\n----\".format(rf_pipe.score(X_train, y_train), rf_pipe.score(X_test, y_test)))\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_pipe.score(X_train, y_train), et_pipe.score(X_test, y_test)))\n",
    "print(\"LSVC Train: {}\\nLSVC Test: {}\\n----\".format(linear_svc_pipe.score(X_train, y_train), linear_svc_pipe.score(X_test, y_test)))\n",
    "print(\"RSVC Train: {}\\nRSVC Test: {}\\n----\".format(rsvc_pipe.score(X_train, y_train), rsvc_pipe.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy scores are about the same so this cleaning probably wasn't necessary but I can feel confident that if some tuning later on would have left our models more reliant on the double newline, now it won't be an issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've been using a count vectorizer so far which has worked best for all the data in labs and lessons we've worked with so far but for the sake of it, let's see how a text frequency vectorizer works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB Train: 0.9332430102229339\n",
      "MNB Test: 0.8248984115256742\n",
      "----\n",
      "BNB Train: 0.6963911811799482\n",
      "BNB Test: 0.6738086442556336\n",
      "----\n",
      "GNB Train: 0.9051607340805518\n",
      "GNB Test: 0.5711119320280753\n",
      "----\n",
      "LogReg Train: 0.9389087326025373\n",
      "LogReg Test: 0.8651643886220909\n",
      "----\n",
      "RF Train: 1.0\n",
      "RF Test: 0.8836350203176949\n",
      "----\n",
      "ET Train: 1.0\n",
      "ET Test: 0.8987809383080901\n",
      "----\n",
      "LSVC Train: 0.973518906269245\n",
      "LSVC Test: 0.8714444033985962\n",
      "----\n",
      "RSVC Train: 0.9959354600320236\n",
      "RSVC Test: 0.8762467676394533\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "mnb_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('densify', FunctionTransformer(lambda x: x.toarray(), accept_sparse = True)),\n",
    "        ('mnb', MultinomialNB())\n",
    "    ]\n",
    ")\n",
    "\n",
    "bnb_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('densify', FunctionTransformer(lambda x: x.toarray(), accept_sparse = True)),\n",
    "        ('bnb', BernoulliNB())\n",
    "    ]\n",
    ")\n",
    "\n",
    "gnb_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('densify', FunctionTransformer(lambda x: x.toarray(), accept_sparse = True)),\n",
    "        ('gnb', GaussianNB())\n",
    "    ]\n",
    ")\n",
    "\n",
    "logr_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('logr', LogisticRegression(max_iter = 500, n_jobs = -1))\n",
    "    ]\n",
    "    )\n",
    "\n",
    "rf_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('rf', RandomForestClassifier(random_state = rs, n_jobs = -1))\n",
    "    ])\n",
    "\n",
    "et_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('et', ExtraTreesClassifier(random_state = rs, n_jobs = -1))\n",
    "    ]\n",
    ")\n",
    "\n",
    "linear_svc_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('lsvc', SVC(kernel = 'linear'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "rsvc_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('rsvc', SVC(kernel = 'rbf'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "mnb_pipe.fit(X_train, y_train)\n",
    "bnb_pipe.fit(X_train, y_train)\n",
    "gnb_pipe.fit(X_train, y_train)\n",
    "logr_pipe.fit(X_train, y_train)\n",
    "rf_pipe.fit(X_train, y_train)\n",
    "et_pipe.fit(X_train, y_train)\n",
    "linear_svc_pipe.fit(X_train, y_train)\n",
    "rsvc_pipe.fit(X_train, y_train)\n",
    "\n",
    "print(\"MNB Train: {}\\nMNB Test: {}\\n----\".format(mnb_pipe.score(X_train, y_train), mnb_pipe.score(X_test, y_test)))\n",
    "print(\"BNB Train: {}\\nBNB Test: {}\\n----\".format(bnb_pipe.score(X_train, y_train), bnb_pipe.score(X_test, y_test)))\n",
    "print(\"GNB Train: {}\\nGNB Test: {}\\n----\".format(gnb_pipe.score(X_train, y_train), gnb_pipe.score(X_test, y_test)))\n",
    "print(\"LogReg Train: {}\\nLogReg Test: {}\\n----\".format(logr_pipe.score(X_train, y_train), logr_pipe.score(X_test, y_test)))\n",
    "print(\"RF Train: {}\\nRF Test: {}\\n----\".format(rf_pipe.score(X_train, y_train), rf_pipe.score(X_test, y_test)))\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_pipe.score(X_train, y_train), et_pipe.score(X_test, y_test)))\n",
    "print(\"LSVC Train: {}\\nLSVC Test: {}\\n----\".format(linear_svc_pipe.score(X_train, y_train), linear_svc_pipe.score(X_test, y_test)))\n",
    "print(\"RSVC Train: {}\\nRSVC Test: {}\\n----\".format(rsvc_pipe.score(X_train, y_train), rsvc_pipe.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a TF-IDF vectorizer instead of a count vectorizer gives slightly worse results for most models though we haven't done any work tuning hyperparameters or scaling so the fact that both are mostly competitive means we'll want to continue to test with both methods. The standout here is Multinomial Naive Bayes which performs 5% better on both sets. No other model saw a change similar but it does seem like if we wanted to consider a Multinomial Naive Bayes model for our final model it would likely use a TF-IDF vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>response</th>\n",
       "      <th>fake</th>\n",
       "      <th>response_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [subreddit, response, fake, response_cleaned]\n",
       "Index: []"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/36216665/find-there-is-an-emoji-in-a-string-in-python3\n",
    "df[(df['response'].str.contains('😀')) & (df['fake'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta Features\n",
    "# number of sentences\n",
    "# average word length\n",
    "# average sentence length\n",
    "# punctuation count (exclude apostrophe?)\n",
    "# newline count? might overlap with sentences\n",
    "# presence of emoji\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
