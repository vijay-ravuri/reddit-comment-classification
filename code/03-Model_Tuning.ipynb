{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "from text_helper import word_splitter, sentence_count, stop_word_counter, punc_counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "from sklearn.compose import make_column_selector, ColumnTransformer\n",
    "\n",
    "import pickle\n",
    "\n",
    "rs = 91923 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/data_final.csv')\n",
    "\n",
    "df['response_cleaned'] = df['response'].apply(lambda x: re.sub('[\\\\n]{2,}', '\\n', x))\n",
    "df['response_cleaned'] = df['response_cleaned'].apply(lambda x: re.sub(r\"\\/*u\\/[\\S]+\", 'they', x))\n",
    "\n",
    "df['num_words'] = df['response_cleaned'].apply(lambda x: word_splitter(x)[0])\n",
    "df['stop_words'] = df['response_cleaned'].apply(stop_word_counter)\n",
    "df['num_sentences'] = df['response_cleaned'].apply(lambda x: sentence_count(x)[0]) \n",
    "df['sentence_length'] = df['response_cleaned'].apply(lambda x: sentence_count(x)[1])\n",
    "df['word_length'] = df['response_cleaned'].apply(lambda x: word_splitter(x)[1])\n",
    "\n",
    "punc_count = df['response_cleaned'].apply(punc_counter)\n",
    "df['punc_ratio'] = punc_count / df['num_words']\n",
    "\n",
    "X = df[['subreddit', 'response_cleaned', 'num_words', 'stop_words', 'num_sentences', 'sentence_length', 'word_length', 'punc_ratio']]\n",
    "y = df['fake']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = rs, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ET Train: 1.0\n",
      "ET Test: 0.9043221278167713\n",
      "----\n",
      "RSVC Train: 0.9125508067495997\n",
      "RSVC Test: 0.8880679719246398\n",
      "----\n",
      "AdaBoost Train: 0.8949378002217022\n",
      "AdaBoost Test: 0.888806797192464\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Recreating our pipelines and our pre-tuning baselines for our three models\n",
    "meta_pipe = Pipeline(\n",
    "    [\n",
    "        ('ss', StandardScaler()) # Since we're done testing with Naive Bayes we can use the default parameters here\n",
    "    ]\n",
    ")\n",
    "\n",
    "text_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer())\n",
    "    ]\n",
    ")\n",
    "\n",
    "subreddit_pipe = Pipeline(\n",
    "    [\n",
    "        ('ohe', OneHotEncoder())\n",
    "    ]\n",
    ")\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "    [\n",
    "        ('meta', meta_pipe, make_column_selector(dtype_include = np.number)),\n",
    "        ('text', text_pipe, 'response_cleaned'),\n",
    "        ('ohe', subreddit_pipe, ['subreddit'])\n",
    "    ],\n",
    "    n_jobs = 6\n",
    ")\n",
    "\n",
    "\n",
    "et_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', col_trans),\n",
    "        ('et', ExtraTreesClassifier(random_state = rs, n_jobs = 6))\n",
    "    ]\n",
    ")\n",
    "\n",
    "rsvc_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', col_trans),\n",
    "        ('rsvc', SVC(kernel = 'rbf'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "adbc_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', col_trans),\n",
    "        ('adbc', AdaBoostClassifier())\n",
    "    ]\n",
    ")\n",
    "\n",
    "et_pipe.fit(X_train, y_train)\n",
    "rsvc_pipe.fit(X_train, y_train)\n",
    "adbc_pipe.fit(X_train, y_train)\n",
    "\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_pipe.score(X_train, y_train), et_pipe.score(X_test, y_test)))\n",
    "print(\"RSVC Train: {}\\nRSVC Test: {}\\n----\".format(rsvc_pipe.score(X_train, y_train), rsvc_pipe.score(X_test, y_test)))\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adbc_pipe.score(X_train, y_train), adbc_pipe.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of our tuning, the above performance levels are our new baseline now that we know for certain we can easily beat the 50% baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deciding between count and TF-IDF vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefining our text_pipe from before to include a standard scaler\n",
    "text_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('ss', StandardScaler(with_mean = False)) # scaling with mean doesn't work on sparse arrays so we'll have to do without\n",
    "    ]\n",
    ")\n",
    "\n",
    "et_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', col_trans),\n",
    "        ('et', ExtraTreesClassifier(random_state = rs, n_jobs = 6))\n",
    "    ]\n",
    ")\n",
    "\n",
    "rsvc_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', col_trans),\n",
    "        ('rsvc', SVC(kernel = 'rbf'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "adbc_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', col_trans),\n",
    "        ('adbc', AdaBoostClassifier())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer()\n",
      "ET Train: 1.0\n",
      "ET Test: 0.9017362393793867\n",
      "----\n",
      "TfidfVectorizer()\n",
      "RSVC Train: 0.970070205690356\n",
      "RSVC Test: 0.6893239748799409\n",
      "----\n",
      "TfidfVectorizer()\n",
      "AdaBoost Train: 0.9022047050129326\n",
      "AdaBoost Test: 0.8766161802733653\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "vect_params = {\n",
    "    'ct__text__vect' : [TfidfVectorizer()]\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "rsvc_grid = GridSearchCV(\n",
    "    rsvc_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(et_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))\n",
    "\n",
    "print(rsvc_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"RSVC Train: {}\\nRSVC Test: {}\\n----\".format(rsvc_grid.score(X_train, y_train), rsvc_grid.score(X_test, y_test)))\n",
    "\n",
    "print(adb_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a slight drop in perfomance going to a standard scaler with TF-IDF vectorizing for Extra Trees and Adaptive Boosting and Radial SVC has dropped dramatically as well. For Radial SVC it seems likely that the standard scaler is the issue so we can retry without that and for the other two let's try both TF-IDF and count vectorizer without a scaler before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer()\n",
      "ET Train: 1.0\n",
      "ET Test: 0.9009974141115626\n",
      "----\n",
      "TfidfVectorizer()\n",
      "RSVC Train: 0.9338588496120211\n",
      "RSVC Test: 0.902844477281123\n",
      "----\n",
      "TfidfVectorizer()\n",
      "AdaBoost Train: 0.9022047050129326\n",
      "AdaBoost Test: 0.8766161802733653\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "text_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer()) # No SS\n",
    "    ]\n",
    ")\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "    [\n",
    "        ('meta', meta_pipe, make_column_selector(dtype_include = np.number)),\n",
    "        ('text', text_pipe, 'response_cleaned'),\n",
    "        ('ohe', subreddit_pipe, ['subreddit'])\n",
    "    ],\n",
    "    n_jobs = 6\n",
    ")\n",
    "\n",
    "et_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', col_trans),\n",
    "        ('et', ExtraTreesClassifier(random_state = rs, n_jobs = 6))\n",
    "    ]\n",
    ")\n",
    "\n",
    "rsvc_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', col_trans),\n",
    "        ('rsvc', SVC(kernel = 'rbf'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "adbc_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', col_trans),\n",
    "        ('adbc', AdaBoostClassifier())\n",
    "    ]\n",
    ")\n",
    "\n",
    "vect_params = {\n",
    "    'ct__text__vect' : [TfidfVectorizer()]\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "rsvc_grid = GridSearchCV(\n",
    "    rsvc_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(et_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))\n",
    "\n",
    "print(rsvc_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"RSVC Train: {}\\nRSVC Test: {}\\n----\".format(rsvc_grid.score(X_train, y_train), rsvc_grid.score(X_test, y_test)))\n",
    "\n",
    "print(adb_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the standardization does bring our SVC model back to its original performance and the TF-IDF vectorizer performs better than our count vectorizer was before. Now with Adaptive Boosting we see TF-IDF is performing better on the training set but slightly worse on the test set. There is also a bit more overfitting going on so let's continue with the count vectorizer here despite the cross-validation prefering TF-IDF vectorizer. For Extra Trees we see something similar but less extreme, TF-IDF performs slightly worse on test data but is prefered by cross-validation over the count vectorizer. Because the performance is so close between the two we will proceed with TF-IDF here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_pipe = Pipeline(\n",
    "    [\n",
    "        ('ss', StandardScaler())\n",
    "    ]\n",
    ")\n",
    "\n",
    "et_text_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('ss', StandardScaler(with_mean = False))\n",
    "    ]\n",
    ")\n",
    "\n",
    "rsvc_text_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', TfidfVectorizer())\n",
    "    ]\n",
    ")\n",
    "\n",
    "adb_text_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer())\n",
    "    ]\n",
    ")\n",
    "\n",
    "subreddit_pipe = Pipeline(\n",
    "    [\n",
    "        ('ohe', OneHotEncoder())\n",
    "    ]\n",
    ")\n",
    "\n",
    "et_col_trans = ColumnTransformer(\n",
    "    [\n",
    "        ('meta', meta_pipe, make_column_selector(dtype_include = np.number)),\n",
    "        ('text', et_text_pipe, 'response_cleaned'),\n",
    "        ('ohe', subreddit_pipe, ['subreddit'])\n",
    "    ],\n",
    "    n_jobs = 6\n",
    ")\n",
    "\n",
    "rsvc_col_trans = ColumnTransformer(\n",
    "    [\n",
    "        ('meta', meta_pipe, make_column_selector(dtype_include = np.number)),\n",
    "        ('text', rsvc_text_pipe, 'response_cleaned'),\n",
    "        ('ohe', subreddit_pipe, ['subreddit'])\n",
    "    ],\n",
    "    n_jobs = 6\n",
    ")\n",
    "\n",
    "adb_col_trans = ColumnTransformer(\n",
    "    [\n",
    "        ('meta', meta_pipe, make_column_selector(dtype_include = np.number)),\n",
    "        ('text', adb_text_pipe, 'response_cleaned'),\n",
    "        ('ohe', subreddit_pipe, ['subreddit'])\n",
    "    ],\n",
    "    n_jobs = 6\n",
    ")\n",
    "\n",
    "et_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', et_col_trans),\n",
    "        ('et', ExtraTreesClassifier(random_state = rs, n_jobs = 6))\n",
    "    ]\n",
    ")\n",
    "\n",
    "rsvc_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', col_trans),\n",
    "        ('rsvc', SVC(kernel = 'rbf'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "adbc_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', col_trans),\n",
    "        ('adbc', AdaBoostClassifier())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking different ngrams and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(ngram_range=(1, 2))\n",
      "ET Train: 1.0\n",
      "ET Test: 0.9050609530845954\n",
      "----\n",
      "CountVectorizer()\n",
      "RSVC Train: 0.9125508067495997\n",
      "RSVC Test: 0.8880679719246398\n",
      "----\n",
      "CountVectorizer()\n",
      "AdaBoost Train: 0.8949378002217022\n",
      "AdaBoost Test: 0.888806797192464\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "vect_params = {\n",
    "    'ct__text__vect__ngram_range' : ((1,1), (1,2), (1,3)),\n",
    "    'ct__text__vect__stop_words' : (None, 'english', stopwords.words('english'))\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "rsvc_grid = GridSearchCV(\n",
    "    rsvc_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(et_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))\n",
    "\n",
    "print(rsvc_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"RSVC Train: {}\\nRSVC Test: {}\\n----\".format(rsvc_grid.score(X_train, y_train), rsvc_grid.score(X_test, y_test)))\n",
    "\n",
    "print(adb_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All models prefer having the stopwords included which was expected, we also see that Extra Trees prefers bigrams included but RSVC and Adaptive Boosting don't get any increased performance out of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(max_features=3000, ngram_range=(1, 2))\n",
      "ET Train: 1.0\n",
      "ET Test: 0.9298115995567049\n",
      "----\n",
      "CountVectorizer(max_features=3000, min_df=10)\n",
      "RSVC Train: 0.9099642813154329\n",
      "RSVC Test: 0.8902844477281123\n",
      "----\n",
      "CountVectorizer(max_features=1000)\n",
      "AdaBoost Train: 0.8949378002217022\n",
      "AdaBoost Test: 0.888806797192464\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "et_vect_params = {\n",
    "    'ct__text__vect__ngram_range' : [(1,2)],\n",
    "    'ct__text__vect__max_features' : (1000, 2000, 3000),\n",
    "    'ct__text__vect__max_df' : (1.0, 0.9, 0.8),\n",
    "    'ct__text__vect__min_df' : (1, 10, 100)\n",
    "}\n",
    "\n",
    "vect_params = {\n",
    "    'ct__text__vect__max_features' : (1000, 2000, 3000),\n",
    "    'ct__text__vect__max_df' : (1.0, 0.9, 0.8),\n",
    "    'ct__text__vect__min_df' : (1, 10, 100)\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = et_vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5, \n",
    "    error_score = 'raise'\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "rsvc_grid = GridSearchCV(\n",
    "    rsvc_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(et_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))\n",
    "\n",
    "print(rsvc_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"RSVC Train: {}\\nRSVC Test: {}\\n----\".format(rsvc_grid.score(X_train, y_train), rsvc_grid.score(X_test, y_test)))\n",
    "\n",
    "print(adb_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each model has its own preferences for hyperparameters so we'll need to tune them separately moving forward. Let's start with Extra Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning TF-IDF Vectorizer for Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(max_df=0.5, max_features=3000, ngram_range=(1, 2))\n",
      "ET Train: 1.0\n",
      "ET Test: 0.9264868858514961\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "et_vect_params = {\n",
    "    'ct__text__vect__ngram_range' : [(1,2)],\n",
    "    'ct__text__vect__max_features' : (2000, 2500, 3000, 4000, 5000),\n",
    "    'ct__text__vect__max_df' : (1.0, 0.5),\n",
    "    'ct__text__vect__min_df' : (1, 0.1, 0.2)\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = et_vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(et_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This best model has very slightly worse test performance but it also has a very extreme value for max_df so let's try and narrow down the true ideal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(max_df=0.7, max_features=2750, ngram_range=(1, 2))\n",
      "ET Train: 1.0\n",
      "ET Test: 0.9301810121906169\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "et_vect_params = {\n",
    "    'ct__text__vect__ngram_range' : [(1,2)],\n",
    "    'ct__text__vect__max_features' : (2750, 3000, 3250, 3500),\n",
    "    'ct__text__vect__max_df' : (1.0, 0.75, 0.7, 0.6)\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = et_vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(et_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(max_df=0.75, max_features=2750, ngram_range=(1, 2))\n",
      "ET Train: 1.0\n",
      "ET Test: 0.925748060583672\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "et_vect_params = {\n",
    "    'ct__text__vect__ngram_range' : [(1,2)],\n",
    "    'ct__text__vect__max_features' : [(2750)],\n",
    "    'ct__text__vect__max_df' : [(0.75)],\n",
    "    'ct__text__vect__min_df' : (1, 10, 100, 0.15)\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = et_vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(et_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks as close to optimal as we can get so let's set aside Extra Trees and move on to Radial SVC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Count Vectorizer for Radial SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(max_features=3000, min_df=25)\n",
      "RSVC Train: 0.9103337849488853\n",
      "RSVC Test: 0.8910232729959364\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "vect_params = {\n",
    "    'ct__text__vect__max_features' : (3000, 5000, 4000, 2500),\n",
    "    'ct__text__vect__min_df' : (10, 50, 75, 25)\n",
    "}\n",
    "\n",
    "rsvc_grid = GridSearchCV(\n",
    "    rsvc_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(rsvc_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"RSVC Train: {}\\nRSVC Test: {}\\n----\".format(rsvc_grid.score(X_train, y_train), rsvc_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(max_features=2750, min_df=30)\n",
      "RSVC Train: 0.9095947776819805\n",
      "RSVC Test: 0.8913926856298485\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "vect_params = {\n",
    "    'ct__text__vect__max_features' : (2750, 3000, 3250, 3500),\n",
    "    'ct__text__vect__min_df' : (15, 20, 25, 30, 40)\n",
    "}\n",
    "\n",
    "rsvc_grid = GridSearchCV(\n",
    "    rsvc_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(rsvc_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"RSVC Train: {}\\nRSVC Test: {}\\n----\".format(rsvc_grid.score(X_train, y_train), rsvc_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems like a good stopping point for RSVC let's move on to Adaptive Boosting before getting into tuning the hyperparameters of our model.s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Count Vectorizer for Adaptive Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(max_features=750, min_df=0.01)\n",
      "AdaBoost Train: 0.8949378002217022\n",
      "AdaBoost Test: 0.888806797192464\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "vect_params = {\n",
    "    'ct__text__vect__max_features' : (500, 750, 1000, 1250, 1500),\n",
    "    'ct__text__vect__min_df' : (0.1, 0.05, 0.01, 0.2)\n",
    "}\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(adb_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(max_features=700, min_df=0.01)\n",
      "AdaBoost Train: 0.8949378002217022\n",
      "AdaBoost Test: 0.888806797192464\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "vect_params = {\n",
    "    'ct__text__vect__max_features' : (650, 700, 750, 800),\n",
    "    'ct__text__vect__min_df' : (0.01, 0.02, 0.005)\n",
    "}\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(adb_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model performance on the test set is about the same but limit features here can help us combat overfitting and keep our model computationally simple which will help while tuning the parameters for Adaptive Boosting later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our parameters for our vectorizers, let's redefine our pipes with the new hyper parameters and get ready to tune hyper parameters for our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_pipe = Pipeline(\n",
    "    [\n",
    "        ('ss', StandardScaler()) # No need to worry about negative values anymore\n",
    "    ]\n",
    ")\n",
    "\n",
    "et_text_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', TfidfVectorizer(max_df = 0.75, max_features = 2750, ngram_range = (1, 2))),\n",
    "        ('ss', StandardScaler(with_mean = False))\n",
    "    ]\n",
    ")\n",
    "\n",
    "rsvc_text_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer(max_features = 2750, min_df = 30))\n",
    "    ]\n",
    ")\n",
    "\n",
    "adb_text_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer(max_features = 700, min_df = 0.01))\n",
    "    ]\n",
    ")\n",
    "\n",
    "subreddit_pipe = Pipeline(\n",
    "    [\n",
    "        ('ohe', OneHotEncoder())\n",
    "    ]\n",
    ")\n",
    "\n",
    "et_col_trans = ColumnTransformer(\n",
    "    [\n",
    "        ('meta', meta_pipe, make_column_selector(dtype_include = np.number)),\n",
    "        ('text', et_text_pipe, 'response_cleaned'),\n",
    "        ('ohe', subreddit_pipe, ['subreddit'])\n",
    "    ],\n",
    "    n_jobs = 6\n",
    ")\n",
    "\n",
    "rsvc_col_trans = ColumnTransformer(\n",
    "    [\n",
    "        ('meta', meta_pipe, make_column_selector(dtype_include = np.number)),\n",
    "        ('text', rsvc_text_pipe, 'response_cleaned'),\n",
    "        ('ohe', subreddit_pipe, ['subreddit'])\n",
    "    ],\n",
    "    n_jobs = 6\n",
    ")\n",
    "\n",
    "adb_col_trans = ColumnTransformer(\n",
    "    [\n",
    "        ('meta', meta_pipe, make_column_selector(dtype_include = np.number)),\n",
    "        ('text', adb_text_pipe, 'response_cleaned'),\n",
    "        ('ohe', subreddit_pipe, ['subreddit'])\n",
    "    ],\n",
    "    n_jobs = 6\n",
    ")\n",
    "\n",
    "et_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', et_col_trans),\n",
    "        ('et', ExtraTreesClassifier(random_state = rs, n_jobs = 6))\n",
    "    ]\n",
    ")\n",
    "\n",
    "rsvc_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', rsvc_col_trans),\n",
    "        ('rsvc', SVC(kernel = 'rbf'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "adbc_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', adb_col_trans),\n",
    "        ('adbc', AdaBoostClassifier())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning hyperparameters for RSVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=5, gamma=0.01)\n",
      "RSVC Train: 0.9945806133760315\n",
      "RSVC Test: 0.9246398226819357\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Used https://www.vebuso.com/2020/03/svm-hyperparameter-tuning-using-gridsearchcv/ along with the documentation to understand these parameters\n",
    "rsvc_params = {\n",
    "    'rsvc__C' : (1.0, 0.1, 10, 0.5, 5), # This parameter is functionally the same as it is for Ridge regression since this is the l2 regularization coefficient\n",
    "    'rsvc__gamma' : ('scale', 'auto', 1.0, 0.1, 0.01, 0.001) # This parameter controls how far away the support vectors we consider can be (lower allows for further vectors)\n",
    "}\n",
    "\n",
    "rsvc_grid = GridSearchCV(\n",
    "    rsvc_pipe,\n",
    "    param_grid = rsvc_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(rsvc_grid.best_estimator_['rsvc'])\n",
    "print(\"RSVC Train: {}\\nRSVC Test: {}\\n----\".format(rsvc_grid.score(X_train, y_train), rsvc_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization has improved this model by quite a bit but we see a lot of overfitting compared to earlier. The default setting for gamma is 'scale' which uses $\\frac{1}{n_features * X.var()}$ which means that its picking an appropriate and already specific value that is scaled to this model. While we are performing better with a gamma of 0.01, this is likely also causing our overfit so let's try continuing without it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=7.5)\n",
      "RSVC Train: 0.9620642936322207\n",
      "RSVC Test: 0.9227927595123753\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "rsvc_params = {\n",
    "    'rsvc__C' : (2.5, 3, 5, 6, 7.5, 9),\n",
    "}\n",
    "\n",
    "rsvc_grid = GridSearchCV(\n",
    "    rsvc_pipe,\n",
    "    param_grid = rsvc_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(rsvc_grid.best_estimator_['rsvc'])\n",
    "print(\"RSVC Train: {}\\nRSVC Test: {}\\n----\".format(rsvc_grid.score(X_train, y_train), rsvc_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see much less overfitting here compared to with gamma of 0.01 but we also see slightly lower test accuracy with our new C. Let's test what C = 5 gives us with the default gamma before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=5)\n",
      "RSVC Train: 0.9533193743071807\n",
      "RSVC Test: 0.9172515700036942\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "rsvc_params = {\n",
    "    'rsvc__C' : [(5)]\n",
    "}\n",
    "\n",
    "rsvc_grid = GridSearchCV(\n",
    "    rsvc_pipe,\n",
    "    param_grid = rsvc_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(rsvc_grid.best_estimator_['rsvc'])\n",
    "print(\"RSVC Train: {}\\nRSVC Test: {}\\n----\".format(rsvc_grid.score(X_train, y_train), rsvc_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this result we can comfortably go with C = 7.5 for the final model. We could continue tuning for a slightly better value but there isn't likely to be much gain. Before we move on we should check the other kernel options just to be safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC()\n",
      "RSVC Train: 0.9095947776819805\n",
      "RSVC Test: 0.8913926856298485\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "rsvc_params = {\n",
    "    'rsvc__kernel' : ('poly', 'rbf', 'sigmoid'),\n",
    "}\n",
    "\n",
    "rsvc_grid = GridSearchCV(\n",
    "    rsvc_pipe,\n",
    "    param_grid = rsvc_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(rsvc_grid.best_estimator_['rsvc'])\n",
    "print(\"RSVC Train: {}\\nRSVC Test: {}\\n----\".format(rsvc_grid.score(X_train, y_train), rsvc_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our lesson on SVM suggested that radial was generally the best option but it doesn't hurt to check. here we see that by default radial performs best so we can proceed with radial and C = 7.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning hyper parameters for Adaptive Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(n_estimators=200)\n",
      "AdaBoost Train: 0.9370612144352752\n",
      "AdaBoost Test: 0.907277428888068\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "adb_params = {\n",
    "    'adbc__n_estimators' : (50, 25, 75, 100, 150, 200), # The number of rounds of boosting, more estimators means more simple models trained in sequence\n",
    "    'adbc__learning_rate' : (1.0, 2.0, 5.0, 10.0, 50.0) # Changes the weight applied to each estimator, a higher rate means each classifier is individually a greater vote\n",
    "}\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = adb_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(adb_grid.best_estimator_['adbc'])\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see our model prefers the original learning rate but let's investigate that further before we move on to further tuning the number of estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(n_estimators=200)\n",
      "AdaBoost Train: 0.9370612144352752\n",
      "AdaBoost Test: 0.907277428888068\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "adb_params = {\n",
    "    'adbc__n_estimators' : (50, 25, 75, 100, 150, 200), \n",
    "    'adbc__learning_rate' : (1.0, 1.1, 1.5, 1.75) \n",
    "}\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = adb_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(adb_grid.best_estimator_['adbc'])\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like slight changes on learning rate are still not changing much so let's set it aside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(n_estimators=400)\n",
      "AdaBoost Train: 0.9552900603522602\n",
      "AdaBoost Test: 0.9157739194680458\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "adb_params = {\n",
    "    'adbc__n_estimators' : (200, 250, 300, 400, 500, 1000)\n",
    "}\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = adb_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(adb_grid.best_estimator_['adbc'])\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(n_estimators=400)\n",
      "AdaBoost Train: 0.9552900603522602\n",
      "AdaBoost Test: 0.9157739194680458\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "adb_params = {\n",
    "    'adbc__n_estimators' : (350, 400, 450, 350, 300)\n",
    "}\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = adb_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(adb_grid.best_estimator_['adbc'])\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we've mostly converged on 400 estimators but let's look a bit deeper before moving on. Let's try some more extreme values for number of estimators because its currently lagging behind our other model's performance. We could be at a soft maximum for performance and be missing a sizable performance gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(n_estimators=400)\n",
      "AdaBoost Train: 0.9552900603522602\n",
      "AdaBoost Test: 0.9157739194680458\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "adb_params = {\n",
    "    'adbc__n_estimators' : (75, 150, 400, 1250, 1500, 1750, 2500, 5000),\n",
    "}\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = adb_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(adb_grid.best_estimator_['adbc'])\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We spent a bit longer tuning just to get a better feel for how the number of estimators and learning rate change our accuracy over time. We hadn't covered this in particular in class so spending the extra time to build that familiarity seemed warranted. One thing that stands out her is that while our test score went up, so did the gap between our training and test splits. This is notable but not surprising since more estimators intuitively should lead to more overfit. Since our perfomance went up a fair bit and these models are doing better through cross-validation we can comfortably accept these are better models. Our best performing model seems to be the one with 400 estimators so we will proceed with that model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning hyper parameters for Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(n_estimators=200, n_jobs=6, random_state=91923)\n",
      "ET Train: 1.0\n",
      "ET Test: 0.9287033616549686\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# There are a LOT of parameters for Extra Trees so tuning many values at once is difficult\n",
    "et_vect_params = {\n",
    "    'et__n_estimators' : (50, 100, 200), # Number of trees\n",
    "    'et__max_depth' : (None, 5, 10), # How deep each tree can be (higher is better at predicting but can lead to overfit)\n",
    "    'et__bootstrap' : (False, True), # Whether or not each tree is trained on bootstrapped data or the original data\n",
    "    'et__max_features' : (None, 'sqrt', 'log2'), # The maximum number of features to use in a given tree\n",
    "    'et__min_samples_split' : (2, 10, 50) # The minimum number of elements in a leaf node\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = et_vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(et_grid.best_estimator_['et'])\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best model seems to be using the highest number of estimators so let's keep increasing there and try a few caps on maximum depth while we're at it to try and combat overfitting. The test accurary is actually a bit lower than the default so we should keep in mind that the default is at least comparable to this model despite being simpler with fewer estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(n_estimators=500, n_jobs=6, random_state=91923)\n",
      "ET Train: 1.0\n",
      "ET Test: 0.9301810121906169\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "et_vect_params = {\n",
    "    'et__n_estimators' : (150, 200, 300, 500),\n",
    "    'et__max_depth' : (None, 15, 50)\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = et_vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(et_grid.best_estimator_['et'])\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we see the highest number of estimators is best and the model does not like having a maximum depth. Let's try more estimators again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(n_estimators=750, n_jobs=6, random_state=91923)\n",
      "ET Train: 1.0\n",
      "ET Test: 0.9290727742888807\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "et_vect_params = {\n",
    "    'et__n_estimators' : (450, 500, 750, 1000),\n",
    "    'et__max_depth' : (None, 15, 25)\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = et_vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(et_grid.best_estimator_['et'])\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've honed in a bit on a prefered number of estimators so let's try the other hyper parameters some more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(max_depth=50, min_samples_split=4, n_estimators=900,\n",
      "                     n_jobs=6, random_state=91923)\n",
      "ET Train: 0.9956891242763887\n",
      "ET Test: 0.9194680458071666\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "et_vect_params = {\n",
    "    'et__n_estimators' : (600, 700, 750, 800, 900),\n",
    "    'et__max_depth' : (5, 15, 25, 50),\n",
    "    'et__min_samples_split' : (2, 4, 6),\n",
    "    'et__min_samples_leaf' : (1, 2, 3)\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = et_vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(et_grid.best_estimator_['et'])\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't seem like this experiment has yielded any good results so let's make one more attempt before we move on. This time we'll limit max depth again but provided more estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(max_depth=75, n_estimators=1250, n_jobs=6,\n",
      "                     random_state=91923)\n",
      "ET Train: 0.9995073284887301\n",
      "ET Test: 0.9287033616549686\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "et_vect_params = {\n",
    "    'et__n_estimators' : (900, 1000, 1250, 1500),\n",
    "    'et__max_depth' : (40, 50, 60, 75)\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = et_vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(et_grid.best_estimator_['et'])\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an interesting result and does bear some further consideration. Since it seems like we can get comparable results this way let's try and improve this further before going back to our best performing model overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(max_depth=100, min_samples_leaf=2, min_samples_split=8,\n",
      "                     n_estimators=1250, n_jobs=6, random_state=91923)\n",
      "ET Train: 0.9954427885207537\n",
      "ET Test: 0.9253786479497599\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "et_vect_params = {\n",
    "    'et__n_estimators' : [(1250)],\n",
    "    'et__max_depth' : (75, 80, 100),\n",
    "    'et__min_samples_split' : (4, 8, 12),\n",
    "    'et__min_samples_leaf' : (2, 5, 7)\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = et_vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(et_grid.best_estimator_['et'])\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point its become evident that this approach is unlikely to show better performance that our earlier model with fewer non-default parameters. Between the 500 and the 750 estimator models we will err on the side that fewer estimators is better within the same tier of performance since they have near identical accuracies. This is in part for the theoretical benefit of avoiding overfit but mainly we want the simpler model that will run a bit faster if only marginally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Pickling Tuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_pipe = Pipeline(\n",
    "    [\n",
    "        ('ss', StandardScaler())\n",
    "    ]\n",
    ")\n",
    "\n",
    "et_text_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', TfidfVectorizer(max_df = 0.75, max_features = 2750, ngram_range = (1, 2))),\n",
    "        ('ss', StandardScaler(with_mean = False))\n",
    "    ]\n",
    ")\n",
    "\n",
    "rsvc_text_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer(max_features = 2750, min_df = 30))\n",
    "    ]\n",
    ")\n",
    "\n",
    "adb_text_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer(max_features = 700, min_df = 0.01))\n",
    "    ]\n",
    ")\n",
    "\n",
    "subreddit_pipe = Pipeline(\n",
    "    [\n",
    "        ('ohe', OneHotEncoder())\n",
    "    ]\n",
    ")\n",
    "\n",
    "et_col_trans = ColumnTransformer(\n",
    "    [\n",
    "        ('meta', meta_pipe, make_column_selector(dtype_include = np.number)),\n",
    "        ('text', et_text_pipe, 'response_cleaned'),\n",
    "        ('ohe', subreddit_pipe, ['subreddit'])\n",
    "    ],\n",
    "    n_jobs = 6\n",
    ")\n",
    "\n",
    "rsvc_col_trans = ColumnTransformer(\n",
    "    [\n",
    "        ('meta', meta_pipe, make_column_selector(dtype_include = np.number)),\n",
    "        ('text', rsvc_text_pipe, 'response_cleaned'),\n",
    "        ('ohe', subreddit_pipe, ['subreddit'])\n",
    "    ],\n",
    "    n_jobs = 6\n",
    ")\n",
    "\n",
    "adb_col_trans = ColumnTransformer(\n",
    "    [\n",
    "        ('meta', meta_pipe, make_column_selector(dtype_include = np.number)),\n",
    "        ('text', adb_text_pipe, 'response_cleaned'),\n",
    "        ('ohe', subreddit_pipe, ['subreddit'])\n",
    "    ],\n",
    "    n_jobs = 6\n",
    ")\n",
    "\n",
    "et_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', et_col_trans),\n",
    "        ('et', ExtraTreesClassifier(n_estimators = 500, random_state = rs, n_jobs = 6))\n",
    "    ]\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "rsvc_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', rsvc_col_trans),\n",
    "        ('rsvc', SVC(kernel = 'rbf', C = 7.5))\n",
    "    ]\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "adbc_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', adb_col_trans),\n",
    "        ('adbc', AdaBoostClassifier(n_estimators = 400))\n",
    "    ]\n",
    ").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pickle_jar/ExtraTrees.pkl', 'wb') as f:\n",
    "    pickle.dump(et_pipe, f)\n",
    "\n",
    "with open('./pickle_jar/RSVC.pkl', 'wb') as f:\n",
    "    pickle.dump(rsvc_pipe, f)\n",
    "\n",
    "with open('./pickle_jar/AdaBoost.pkl', 'wb') as f:\n",
    "    pickle.dump(adbc_pipe, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
