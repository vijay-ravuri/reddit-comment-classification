{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "from text_helper import word_splitter, sentence_count, stop_word_counter, punc_counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "from sklearn.compose import make_column_selector, ColumnTransformer\n",
    "\n",
    "import pickle\n",
    "\n",
    "rs = 91923 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/data_final.csv')\n",
    "\n",
    "df['response_cleaned'] = df['response'].apply(lambda x: re.sub('[\\\\n]{2,}', '\\n', x))\n",
    "df['response_cleaned'] = df['response_cleaned'].apply(lambda x: re.sub(r\"\\/*u\\/[\\S]+\", 'they', x))\n",
    "\n",
    "df['num_words'] = df['response_cleaned'].apply(lambda x: word_splitter(x)[0])\n",
    "df['stop_words'] = df['response_cleaned'].apply(stop_word_counter)\n",
    "df['num_sentences'] = df['response_cleaned'].apply(lambda x: sentence_count(x)[0]) \n",
    "df['sentence_length'] = df['response_cleaned'].apply(lambda x: sentence_count(x)[1])\n",
    "df['word_length'] = df['response_cleaned'].apply(lambda x: word_splitter(x)[1])\n",
    "\n",
    "punc_count = df['response_cleaned'].apply(punc_counter)\n",
    "df['punc_ratio'] = punc_count / df['num_words']\n",
    "\n",
    "X = df[['subreddit', 'response_cleaned', 'num_words', 'stop_words', 'num_sentences', 'sentence_length', 'word_length', 'punc_ratio']]\n",
    "y = df['fake']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = rs, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ET Train: 1.0\n",
      "ET Test: 0.9043221278167713\n",
      "----\n",
      "RSVC Train: 0.9125508067495997\n",
      "RSVC Test: 0.8880679719246398\n",
      "----\n",
      "AdaBoost Train: 0.8949378002217022\n",
      "AdaBoost Test: 0.888806797192464\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Recreating out pipelines and our pre-tuning baselines for our three models\n",
    "meta_pipe = Pipeline(\n",
    "    [\n",
    "        ('ss', StandardScaler()) # Since we're done testing with Naive Bayes we can use the default parameters here\n",
    "    ]\n",
    ")\n",
    "\n",
    "text_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer())\n",
    "    ]\n",
    ")\n",
    "\n",
    "subreddit_pipe = Pipeline(\n",
    "    [\n",
    "        ('ohe', OneHotEncoder())\n",
    "    ]\n",
    ")\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "    [\n",
    "        ('meta', meta_pipe, make_column_selector(dtype_include = np.number)),\n",
    "        ('text', text_pipe, 'response_cleaned'),\n",
    "        ('ohe', subreddit_pipe, ['subreddit'])\n",
    "    ],\n",
    "    n_jobs = 6\n",
    ")\n",
    "\n",
    "\n",
    "et_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', col_trans),\n",
    "        ('et', ExtraTreesClassifier(random_state = rs, n_jobs = 6))\n",
    "    ]\n",
    ")\n",
    "\n",
    "rsvc_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', col_trans),\n",
    "        ('rsvc', SVC(kernel = 'rbf'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "adbc_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', col_trans),\n",
    "        ('adbc', AdaBoostClassifier())\n",
    "    ]\n",
    ")\n",
    "\n",
    "et_pipe.fit(X_train, y_train)\n",
    "rsvc_pipe.fit(X_train, y_train)\n",
    "adbc_pipe.fit(X_train, y_train)\n",
    "\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_pipe.score(X_train, y_train), et_pipe.score(X_test, y_test)))\n",
    "print(\"RSVC Train: {}\\nRSVC Test: {}\\n----\".format(rsvc_pipe.score(X_train, y_train), rsvc_pipe.score(X_test, y_test)))\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adbc_pipe.score(X_train, y_train), adbc_pipe.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefining our text_pipe from before to include a standard scaler\n",
    "text_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('ss', StandardScaler(with_mean = False)) # scaling with mean doesn't work on sparse arrays so we'll have to do without\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ET Train: 1.0\n",
      "ET Test: 0.9009974141115626\n",
      "----\n",
      "RSVC Train: 0.9338588496120211\n",
      "RSVC Test: 0.902844477281123\n",
      "----\n",
      "AdaBoost Train: 0.9022047050129326\n",
      "AdaBoost Test: 0.8766161802733653\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "vect_params = {\n",
    "    'ct__text__vect' : [TfidfVectorizer()]\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "rsvc_grid = GridSearchCV(\n",
    "    rsvc_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))\n",
    "print(\"RSVC Train: {}\\nRSVC Test: {}\\n----\".format(rsvc_grid.score(X_train, y_train), rsvc_grid.score(X_test, y_test)))\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer()\n",
      "TfidfVectorizer()\n",
      "CountVectorizer()\n"
     ]
    }
   ],
   "source": [
    "print(et_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(rsvc_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(adb_grid.best_estimator_['ct'].transformers[1][1]['vect'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our tree-based models prefer a count vectorizer but our support vector classifier performs better with the TF-IDF vectorizer. However, we see a sharp performance drop off for the support vector model now that our data is scaled which is disconcerting. Our actual performance for the other two models is identical with or without scaling so let's not scale since it doesn't give us anything it seems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_pipe = Pipeline(\n",
    "    [\n",
    "        ('ss', StandardScaler()) # No need to worry about negative values anymore\n",
    "    ]\n",
    ")\n",
    "\n",
    "text_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer()) # Removing scaling\n",
    "    ]\n",
    ")\n",
    "\n",
    "subreddit_pipe = Pipeline(\n",
    "    [\n",
    "        ('ohe', OneHotEncoder())\n",
    "    ]\n",
    ")\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "    [\n",
    "        ('meta', meta_pipe, make_column_selector(dtype_include = np.number)),\n",
    "        ('text', text_pipe, 'response_cleaned'),\n",
    "        ('ohe', subreddit_pipe, ['subreddit'])\n",
    "    ],\n",
    "    n_jobs = 6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ET Train: 1.0\n",
      "ET Test: 0.9128186183967492\n",
      "----\n",
      "RSVC Train: 0.9100874491932504\n",
      "RSVC Test: 0.8862209087550794\n",
      "----\n",
      "AdaBoost Train: 0.8949378002217022\n",
      "AdaBoost Test: 0.888806797192464\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "vect_params = {\n",
    "    'ct__text__vect__ngram_range' : ((1,1), (1,2), (1,3)),\n",
    "    'ct__text__vect__stop_words' : (None, 'english', stopwords.words('english'))\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "rsvc_grid = GridSearchCV(\n",
    "    rsvc_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))\n",
    "print(\"RSVC Train: {}\\nRSVC Test: {}\\n----\".format(rsvc_grid.score(X_train, y_train), rsvc_grid.score(X_test, y_test)))\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(ngram_range=(1, 2))\n",
      "CountVectorizer()\n",
      "CountVectorizer()\n"
     ]
    }
   ],
   "source": [
    "print(et_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(rsvc_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(adb_grid.best_estimator_['ct'].transformers[1][1]['vect'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, only our Extra Trees models prefers the non-default parameters. Both Radial SVC and Adaptive Boosting prefer no stop words excluded and no ngrams (single sets of words instead of pairs). Extra Trees prefers bigrams included but otherwise still prefers not removing stop words. This is to be expected since we saw a large difference in stop word usage for real comments vs AI generated ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(max_features=3000, ngram_range=(1, 2))\n",
      "ET Train: 1.0\n",
      "ET Test: 0.9275951237532324\n",
      "----\n",
      "CountVectorizer(max_features=3000, min_df=10)\n",
      "RSVC Train: 0.9099642813154329\n",
      "RSVC Test: 0.8902844477281123\n",
      "----\n",
      "CountVectorizer(max_features=1000)\n",
      "AdaBoost Train: 0.8949378002217022\n",
      "AdaBoost Test: 0.888806797192464\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "et_vect_params = {\n",
    "    'ct__text__vect__ngram_range' : [(1,2)],\n",
    "    'ct__text__vect__max_features' : (1000, 2000, 3000),\n",
    "    'ct__text__vect__max_df' : (1.0, 0.9, 0.8),\n",
    "    'ct__text__vect__min_df' : (1, 10, 100)\n",
    "}\n",
    "\n",
    "vect_params = {\n",
    "    'ct__text__vect__max_features' : (1000, 2000, 3000),\n",
    "    'ct__text__vect__max_df' : (1.0, 0.9, 0.8),\n",
    "    'ct__text__vect__min_df' : (1, 10, 100)\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = et_vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5, \n",
    "    error_score = 'raise'\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "rsvc_grid = GridSearchCV(\n",
    "    rsvc_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(et_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))\n",
    "print(rsvc_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"RSVC Train: {}\\nRSVC Test: {}\\n----\".format(rsvc_grid.score(X_train, y_train), rsvc_grid.score(X_test, y_test)))\n",
    "print(adb_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each model has its own preferences for hyperparamters so we'll need to tune them separately moving forward. Let's start with Extra Trees since its currently doing the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Count Vectorizer for Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(max_features=4000, ngram_range=(1, 2))\n",
      "ET Train: 1.0\n",
      "ET Test: 0.9268562984854082\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "et_vect_params = {\n",
    "    'ct__text__vect__ngram_range' : [(1,2)],\n",
    "    'ct__text__vect__max_features' : (2000, 2500, 3000, 4000, 5000),\n",
    "    'ct__text__vect__max_df' : (1.0, 0.5),\n",
    "    'ct__text__vect__min_df' : (1, 0.1, 0.2)\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = et_vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(et_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like no amount of fiddling with min and max document frequency is helping here so let's narrow in on what the best value of maximum features is. As a note, we see that while 4000 features is better by cross-validation, 3000 is slightly better on the actual test set. Since the two values are close, let's err on the side of trusting cross-validation to have selected the more robust hyper parameter value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(max_features=4000, ngram_range=(1, 2))\n",
      "ET Train: 1.0\n",
      "ET Test: 0.9268562984854082\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "et_vect_params = {\n",
    "    'ct__text__vect__ngram_range' : [(1,2)],\n",
    "    'ct__text__vect__max_features' : (3500, 3750, 4000, 4250, 4500)\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = et_vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(et_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(max_features=4000, ngram_range=(1, 2))\n",
      "ET Train: 1.0\n",
      "ET Test: 0.9268562984854082\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "et_vect_params = {\n",
    "    'ct__text__vect__ngram_range' : [(1,2)],\n",
    "    'ct__text__vect__max_features' : (3750, 3800, 3900, 4000, 4100, 4200, 4250)\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = et_vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(et_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks as close to optimal as we can get so let's set aside Extra Trees and move on to Radial SVC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Count Vectorizer for Radial SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(max_features=2500, min_df=30)\n",
      "RSVC Train: 0.9095947776819805\n",
      "RSVC Test: 0.8913926856298485\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "vect_params = {\n",
    "    'ct__text__vect__max_features' : (2500, 3000, 4000, 5000),\n",
    "    'ct__text__vect__min_df' : (5, 10, 20, 30)\n",
    "}\n",
    "\n",
    "rsvc_grid = GridSearchCV(\n",
    "    rsvc_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(rsvc_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"RSVC Train: {}\\nRSVC Test: {}\\n----\".format(rsvc_grid.score(X_train, y_train), rsvc_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(max_features=2250, min_df=30)\n",
      "RSVC Train: 0.9094716098041631\n",
      "RSVC Test: 0.8913926856298485\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "vect_params = {\n",
    "    'ct__text__vect__max_features' : (2250, 2500, 2750),\n",
    "    'ct__text__vect__min_df' : (25, 30, 35, 40)\n",
    "}\n",
    "\n",
    "rsvc_grid = GridSearchCV(\n",
    "    rsvc_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(rsvc_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"RSVC Train: {}\\nRSVC Test: {}\\n----\".format(rsvc_grid.score(X_train, y_train), rsvc_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems like a good stopping point for RSVC let's move on to Adaptive Boosting before getting into tuning the hyperparameters of our model.s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Count Vectorizer for Adaptive Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(max_features=750)\n",
      "AdaBoost Train: 0.8949378002217022\n",
      "AdaBoost Test: 0.888806797192464\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "vect_params = {\n",
    "    'ct__text__vect__max_features' : (500, 750, 1000, 1250, 1500),\n",
    "    'ct__text__vect__max_df' : (1.0, 0.7, 0.6, 0.5),\n",
    "    'ct__text__vect__min_df' : (1, 10, 100)\n",
    "}\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(adb_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(max_features=700)\n",
      "AdaBoost Train: 0.8949378002217022\n",
      "AdaBoost Test: 0.888806797192464\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "vect_params = {\n",
    "    'ct__text__vect__max_features' : (700, 750, 800)\n",
    "}\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(adb_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(max_features=700)\n",
      "AdaBoost Train: 0.8949378002217022\n",
      "AdaBoost Test: 0.888806797192464\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "vect_params = {\n",
    "    'ct__text__vect__max_features' : (600, 650, 700)\n",
    "}\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(adb_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptive Boosting seems about stable with as few as 700 features at most and performs best with the default values for min and max document frequency. Intuitively, this makes sense because the model isn't taking a huge number of features so its not trending towards overfitting and needing to be corralled in by limiting which features it can take. Out of pure curiosity, does no limit to max features perform better or worse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer()\n",
      "AdaBoost Train: 0.8949378002217022\n",
      "AdaBoost Test: 0.888806797192464\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "vect_params = {\n",
    "    'ct__text__vect__max_features' : [(None)]\n",
    "}\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(adb_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this we can definitively see that Adaptive Boosting does not need a huge number of features in order to reach its maximum predictive power, even leaving it uncapped we end up with the same performance as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our Count Vectorizer parameters, let's redefine our pipes with the new hyper parameters and get ready to tune hyperparameters for our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_text_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer(max_features = 4000, ngram_range = (1, 2)))\n",
    "    ]\n",
    ")\n",
    "\n",
    "rsvc_text_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer(max_features = 2250, min_df = 30))\n",
    "    ]\n",
    ")\n",
    "\n",
    "adb_text_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer(max_features = 700))\n",
    "    ]\n",
    ")\n",
    "\n",
    "et_col_trans = ColumnTransformer(\n",
    "    [\n",
    "        ('meta', meta_pipe, make_column_selector(dtype_include = np.number)),\n",
    "        ('text', et_text_pipe, 'response_cleaned'),\n",
    "        ('ohe', subreddit_pipe, ['subreddit'])\n",
    "    ],\n",
    "    n_jobs = 6\n",
    ")\n",
    "\n",
    "rsvc_col_trans = ColumnTransformer(\n",
    "    [\n",
    "        ('meta', meta_pipe, make_column_selector(dtype_include = np.number)),\n",
    "        ('text', rsvc_text_pipe, 'response_cleaned'),\n",
    "        ('ohe', subreddit_pipe, ['subreddit'])\n",
    "    ],\n",
    "    n_jobs = 6\n",
    ")\n",
    "\n",
    "adb_col_trans = ColumnTransformer(\n",
    "    [\n",
    "        ('meta', meta_pipe, make_column_selector(dtype_include = np.number)),\n",
    "        ('text', adb_text_pipe, 'response_cleaned'),\n",
    "        ('ohe', subreddit_pipe, ['subreddit'])\n",
    "    ],\n",
    "    n_jobs = 6\n",
    ")\n",
    "\n",
    "et_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', et_col_trans),\n",
    "        ('et', ExtraTreesClassifier(random_state = rs, n_jobs = 6))\n",
    "    ]\n",
    ")\n",
    "\n",
    "rsvc_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', rsvc_col_trans),\n",
    "        ('rsvc', SVC(kernel = 'rbf'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "adbc_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', adb_col_trans),\n",
    "        ('adbc', AdaBoostClassifier())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning hyperparameters for RSVC\n",
    "\n",
    "Let's start with our SVC model because it has the fewest hyperparameters and the worst performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=5, gamma=0.01)\n",
      "RSVC Train: 0.994457445498214\n",
      "RSVC Test: 0.9224233468784633\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Used https://www.vebuso.com/2020/03/svm-hyperparameter-tuning-using-gridsearchcv/ along with the documentation to understand these parameters\n",
    "rsvc_params = {\n",
    "    'rsvc__C' : (1.0, 0.1, 10, 0.5, 5), # This parameter is functionally the same as it is for Ridge regression since this is the l2 regularization coefficient\n",
    "    'rsvc__gamma' : ('scale', 'auto', 1.0, 0.1, 0.01, 0.001) # This parameter controls how far away the support vectors we consider can be (lower allows for further vectors)\n",
    "}\n",
    "\n",
    "rsvc_grid = GridSearchCV(\n",
    "    rsvc_pipe,\n",
    "    param_grid = rsvc_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(rsvc_grid.best_estimator_['rsvc'])\n",
    "print(\"RSVC Train: {}\\nRSVC Test: {}\\n----\".format(rsvc_grid.score(X_train, y_train), rsvc_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization has improved this model by quite a bit and we see that a gamma of 0.1 is currently preferred. The default setting of 'scale' uses $\\frac{1}{n_features * X.var()}$ which means that its picking an appropriate and already specific value that is specific to this model. Let's continue this approach a bit longer and see if we get anything noticably better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=5, gamma=0.05)\n",
      "RSVC Train: 0.9996304963665475\n",
      "RSVC Test: 0.8282231252308829\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "rsvc_params = {\n",
    "    'rsvc__C' : (2.5, 5, 7.5), \n",
    "    'rsvc__gamma' : (0.1, 0.2, 0.5, 0.05) \n",
    "}\n",
    "\n",
    "rsvc_grid = GridSearchCV(\n",
    "    rsvc_pipe,\n",
    "    param_grid = rsvc_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(rsvc_grid.best_estimator_['rsvc'])\n",
    "print(\"RSVC Train: {}\\nRSVC Test: {}\\n----\".format(rsvc_grid.score(X_train, y_train), rsvc_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best model here actually performs much worse on the test set than our prior best model. This is quite interesting and will need to be kept in mind moving forward. Let's try another round of tuning but if we don't see a marked improvement over our first round we will take that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=6, gamma=0.025)\n",
      "RSVC Train: 0.9988914890996428\n",
      "RSVC Test: 0.8799408939785741\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "rsvc_params = {\n",
    "    'rsvc__C' : (4, 5, 6), \n",
    "    'rsvc__gamma' : (0.075, 0.05, 0.025) \n",
    "}\n",
    "\n",
    "rsvc_grid = GridSearchCV(\n",
    "    rsvc_pipe,\n",
    "    param_grid = rsvc_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(rsvc_grid.best_estimator_['rsvc'])\n",
    "print(\"RSVC Train: {}\\nRSVC Test: {}\\n----\".format(rsvc_grid.score(X_train, y_train), rsvc_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we see the overfit issue here. My current intuition is that shrinking gamma is leading to this overfit so let's move back to the default gamma and find the best C for that value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=10)\n",
      "RSVC Train: 0.9676068481340067\n",
      "RSVC Test: 0.9231621721462874\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "rsvc_params = {\n",
    "    'rsvc__C' : (2.5, 5, 10, 15)\n",
    "}\n",
    "\n",
    "rsvc_grid = GridSearchCV(\n",
    "    rsvc_pipe,\n",
    "    param_grid = rsvc_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(rsvc_grid.best_estimator_['rsvc'])\n",
    "print(\"RSVC Train: {}\\nRSVC Test: {}\\n----\".format(rsvc_grid.score(X_train, y_train), rsvc_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=10)\n",
      "RSVC Train: 0.9676068481340067\n",
      "RSVC Test: 0.9231621721462874\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "rsvc_params = {\n",
    "    'rsvc__C' : (7.5, 10, 12.5)\n",
    "}\n",
    "\n",
    "rsvc_grid = GridSearchCV(\n",
    "    rsvc_pipe,\n",
    "    param_grid = rsvc_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(rsvc_grid.best_estimator_['rsvc'])\n",
    "print(\"RSVC Train: {}\\nRSVC Test: {}\\n----\".format(rsvc_grid.score(X_train, y_train), rsvc_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like with default scale value our model performs best with C = 10. We do see less overfit for the same predictive power as our best gamma-tuned model so let's proceed with this as the final RSVC model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning hyper parameters for Adaptive Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(n_estimators=200)\n",
      "AdaBoost Train: 0.9370612144352752\n",
      "AdaBoost Test: 0.907277428888068\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "adb_params = {\n",
    "    'adbc__n_estimators' : (50, 25, 75, 100, 150, 200), # The number of rounds of boosting, more estimators means more simple models trained in sequence\n",
    "    'adbc__learning_rate' : (1.0, 2.0, 5.0, 10.0, 50.0) # Changes the weight applied to each estimator, a higher rate means each classifier is individually a greater vote\n",
    "}\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = adb_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(adb_grid.best_estimator_['adbc'])\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see our model prefers the original learning rate but let's investigate that further before we move on to further tuning the number of estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(n_estimators=200)\n",
      "AdaBoost Train: 0.9370612144352752\n",
      "AdaBoost Test: 0.907277428888068\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "adb_params = {\n",
    "    'adbc__n_estimators' : (50, 25, 75, 100, 150, 200), \n",
    "    'adbc__learning_rate' : (1.0, 1.1, 1.5, 1.75) \n",
    "}\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = adb_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(adb_grid.best_estimator_['adbc'])\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we can feel comfortable setting learning rate aside and instead tuning n_estimators further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(n_estimators=200)\n",
      "AdaBoost Train: 0.9370612144352752\n",
      "AdaBoost Test: 0.907277428888068\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "adb_params = {\n",
    "    'adbc__n_estimators' : (175, 190, 200, 225, 250)\n",
    "}\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = adb_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(adb_grid.best_estimator_['adbc'])\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(n_estimators=500)\n",
      "AdaBoost Train: 0.960832614854046\n",
      "AdaBoost Test: 0.9183598079054304\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "adb_params = {\n",
    "    'adbc__n_estimators' : (200, 250, 300, 400, 500, 1000)\n",
    "}\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = adb_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(adb_grid.best_estimator_['adbc'])\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(n_estimators=500)\n",
      "AdaBoost Train: 0.960832614854046\n",
      "AdaBoost Test: 0.9183598079054304\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "adb_params = {\n",
    "    'adbc__n_estimators' : (450, 500, 600, 700)\n",
    "}\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = adb_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(adb_grid.best_estimator_['adbc'])\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(n_estimators=475)\n",
      "AdaBoost Train: 0.9602167754649588\n",
      "AdaBoost Test: 0.9216845216106391\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "adb_params = {\n",
    "    'adbc__n_estimators' : (475, 500, 525, 550)\n",
    "}\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = adb_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(adb_grid.best_estimator_['adbc'])\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We spent a bit longer tuning just to get a better feel for how the n_estimators parameter changes our accuracy over time. We hadn't covered this in particular in class so spending the extra time to build that familiarity seemed warranted. One thing that stands out her is that while our test score went up, so did the gap between our training and test splits. This is notable but not surprising since more estimators intuitively should lead to more overfit. Since our perfomance went up a lot and these models are doing better through cross-validation we can comfortably accept this is the best model. Let's move on to Extra Trees and see if we can get the performance any better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning hyper parameters for Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(max_features=4000, ngram_range=(1, 2))\n",
      "ET Train: 1.0\n",
      "ET Test: 0.9268562984854082\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# There are a LOT of parameters for Extra Trees so tuning many values at once is difficult\n",
    "et_vect_params = {\n",
    "    'et__n_estimators' : (50, 100, 200), # Number of trees\n",
    "    'et__max_depth' : (None, 5, 10), # How deep each tree can be (higher is better at predicting but can lead to overfit)\n",
    "    'et__bootstrap' : (False, True), # Whether or not each tree is trained on bootstrapped data or the original data\n",
    "    'et__max_features' : (None, 'sqrt', 'log2'), # The maximum number of features to use in a given tree\n",
    "    'et__min_samples_split' : (2, 10, 50) # The minimum number of elements in a leaf node\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = et_vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(et_grid.best_estimator_['et'])# print(et_grid.best_estimator_['ct'].transformers[1][1]['vect']) This was a typo and shows the wrong info, unfortunately this took 35 minutes to run so it may just be staying here\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(n_jobs=6, random_state=91923)\n"
     ]
    }
   ],
   "source": [
    "print(et_grid.best_estimator_['et'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far our best model is the original one. Let's narrow down only on a couple of parameters and try again. We'll focus on n_estimators and max_depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(n_jobs=6, random_state=91923)\n",
      "ET Train: 1.0\n",
      "ET Test: 0.9268562984854082\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "et_vect_params = {\n",
    "    'et__n_estimators' : (75, 100, 125),\n",
    "    'et__max_depth' : (None, 15, 50)\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = et_vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(et_grid.best_estimator_['et'])\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we don't see any gains so let's change gears and look into the hyper parameters that control when and if we create a further split, min_samples_split and min_samples_leaf. These control the number of samples required for a split to happen and the minimum number of observations in each leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(n_jobs=6, random_state=91923)\n",
      "ET Train: 1.0\n",
      "ET Test: 0.9268562984854082\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "et_vect_params = {\n",
    "    'et__min_samples_split' : (2, 10, 50),\n",
    "    'et__min_samples_leaf' : (1, 5, 25)\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = et_vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(et_grid.best_estimator_['et'])\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(n_jobs=6, random_state=91923)\n",
      "ET Train: 1.0\n",
      "ET Test: 0.9268562984854082\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "et_vect_params = {\n",
    "    'et__min_samples_split' : (2, 4, 6),\n",
    "    'et__min_samples_leaf' : (1, 2, 3)\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = et_vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(et_grid.best_estimator_['et'])\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we can concede that no tuning is particularly helping our model. That being said, very likely this is caused by our training accuracy being perfect and if that is consistent across a number of other models then many different ones can be equally effective but technically more robust to overfitting issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(max_depth=50, n_estimators=200, n_jobs=6,\n",
      "                     random_state=91923)\n",
      "ET Train: 0.9953196206429363\n",
      "ET Test: 0.9272257111193203\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "et_vect_params = {\n",
    "    'et__n_estimators' : (75, 125, 200),\n",
    "    'et__max_depth' : (5, 10, 15, 50)\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = et_vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(et_grid.best_estimator_['et'])\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like it actually worked, lesson learned default values lead to overfit and can cause better models to be 'hidden'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(max_depth=75, n_estimators=300, n_jobs=6,\n",
      "                     random_state=91923)\n",
      "ET Train: 0.9993841606109126\n",
      "ET Test: 0.9335057258958256\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "et_vect_params = {\n",
    "    'et__n_estimators' : (150, 200, 250, 300),\n",
    "    'et__max_depth' : (25, 50, 75)\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = et_vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(et_grid.best_estimator_['et'])\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(max_depth=80, n_estimators=500, n_jobs=6,\n",
      "                     random_state=91923)\n",
      "ET Train: 0.9995073284887301\n",
      "ET Test: 0.9331363132619136\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "et_vect_params = {\n",
    "    'et__n_estimators' : (300, 400, 500),\n",
    "    'et__max_depth' : (70, 75, 80)\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = et_vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(et_grid.best_estimator_['et'])\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not seeing any gains really here and its quite hard to decide which model to use. For this reason, let's choose the model we trained with the lowest overfit even if only slightl because we cannot guarantee that further gains, or even this far in the first place, are truly better than what we had before. We're going to proceed with the 75 max depth and 300 estimators model for the above reasons. The model is still very overfit but that comes with the territory as we saw near perfect training accuraccy across the board."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Models and Pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', et_col_trans),\n",
    "        ('et', ExtraTreesClassifier(max_depth = 75, n_estimators = 300, random_state = rs, n_jobs = 6))\n",
    "    ]\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "rsvc_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', rsvc_col_transcol_trans),\n",
    "        ('rsvc', SVC(kernel = 'rbf', C = 10))\n",
    "    ]\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "adbc_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', adb_col_trans),\n",
    "        ('adbc', AdaBoostClassifier(n_estimators = 475))\n",
    "    ]\n",
    ").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pickle_jar/ExtraTrees.pkl', 'wb') as f:\n",
    "    pickle.dump(et_pipe, f)\n",
    "\n",
    "with open('./pickle_jar/RSVC.pkl', 'wb') as f:\n",
    "    pickle.dump(rsvc_pipe, f)\n",
    "\n",
    "with open('./pickle_jar/AdaBoost.pkl', 'wb') as f:\n",
    "    pickle.dump(adbc_pipe, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
