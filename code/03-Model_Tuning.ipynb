{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "from text_helper import word_splitter, sentence_count, stop_word_counter, punc_counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "from sklearn.compose import make_column_selector, ColumnTransformer\n",
    "\n",
    "import pickle\n",
    "\n",
    "rs = 91923 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/data_final.csv')\n",
    "\n",
    "df['response_cleaned'] = df['response'].apply(lambda x: re.sub('[\\\\n]{2,}', '\\n', x))\n",
    "df['response_cleaned'] = df['response_cleaned'].apply(lambda x: re.sub(r\"\\/*u\\/[\\S]+\", 'they', x))\n",
    "\n",
    "df['num_words'] = df['response_cleaned'].apply(lambda x: word_splitter(x)[0])\n",
    "df['stop_words'] = df['response_cleaned'].apply(stop_word_counter)\n",
    "df['num_sentences'] = df['response_cleaned'].apply(lambda x: sentence_count(x)[0]) \n",
    "df['sentence_length'] = df['response_cleaned'].apply(lambda x: sentence_count(x)[1])\n",
    "df['word_length'] = df['response_cleaned'].apply(lambda x: word_splitter(x)[1])\n",
    "\n",
    "punc_count = df['response_cleaned'].apply(punc_counter)\n",
    "df['punc_ratio'] = punc_count / df['num_words']\n",
    "\n",
    "X = df[['subreddit', 'response_cleaned', 'num_words', 'stop_words', 'num_sentences', 'sentence_length', 'word_length', 'punc_ratio']]\n",
    "y = df['fake']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = rs, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ET Train: 1.0\n",
      "ET Test: 0.9043221278167713\n",
      "----\n",
      "RSVC Train: 0.9125508067495997\n",
      "RSVC Test: 0.8880679719246398\n",
      "----\n",
      "AdaBoost Train: 0.8949378002217022\n",
      "AdaBoost Test: 0.888806797192464\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Recreating our pipelines and our pre-tuning baselines for our three models\n",
    "meta_pipe = Pipeline(\n",
    "    [\n",
    "        ('ss', StandardScaler()) # Since we're done testing with Naive Bayes we can use the default parameters here\n",
    "    ]\n",
    ")\n",
    "\n",
    "text_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer())\n",
    "    ]\n",
    ")\n",
    "\n",
    "subreddit_pipe = Pipeline(\n",
    "    [\n",
    "        ('ohe', OneHotEncoder())\n",
    "    ]\n",
    ")\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "    [\n",
    "        ('meta', meta_pipe, make_column_selector(dtype_include = np.number)),\n",
    "        ('text', text_pipe, 'response_cleaned'),\n",
    "        ('ohe', subreddit_pipe, ['subreddit'])\n",
    "    ],\n",
    "    n_jobs = 6\n",
    ")\n",
    "\n",
    "\n",
    "et_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', col_trans),\n",
    "        ('et', ExtraTreesClassifier(random_state = rs, n_jobs = 6))\n",
    "    ]\n",
    ")\n",
    "\n",
    "rsvc_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', col_trans),\n",
    "        ('rsvc', SVC(kernel = 'rbf'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "adbc_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', col_trans),\n",
    "        ('adbc', AdaBoostClassifier())\n",
    "    ]\n",
    ")\n",
    "\n",
    "et_pipe.fit(X_train, y_train)\n",
    "rsvc_pipe.fit(X_train, y_train)\n",
    "adbc_pipe.fit(X_train, y_train)\n",
    "\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_pipe.score(X_train, y_train), et_pipe.score(X_test, y_test)))\n",
    "print(\"RSVC Train: {}\\nRSVC Test: {}\\n----\".format(rsvc_pipe.score(X_train, y_train), rsvc_pipe.score(X_test, y_test)))\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adbc_pipe.score(X_train, y_train), adbc_pipe.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefining our text_pipe from before to include a standard scaler\n",
    "text_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('ss', StandardScaler(with_mean = False)) # scaling with mean doesn't work on sparse arrays so we'll have to do without\n",
    "    ]\n",
    ")\n",
    "\n",
    "et_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', col_trans),\n",
    "        ('et', ExtraTreesClassifier(random_state = rs, n_jobs = 6))\n",
    "    ]\n",
    ")\n",
    "\n",
    "rsvc_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', col_trans),\n",
    "        ('rsvc', SVC(kernel = 'rbf'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "adbc_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', col_trans),\n",
    "        ('adbc', AdaBoostClassifier())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer()\n",
      "ET Train: 1.0\n",
      "ET Test: 0.9017362393793867\n",
      "----\n",
      "TfidfVectorizer()\n",
      "RSVC Train: 0.970070205690356\n",
      "RSVC Test: 0.6893239748799409\n",
      "----\n",
      "TfidfVectorizer()\n",
      "AdaBoost Train: 0.9022047050129326\n",
      "AdaBoost Test: 0.8766161802733653\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "vect_params = {\n",
    "    'ct__text__vect' : [TfidfVectorizer()]\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "rsvc_grid = GridSearchCV(\n",
    "    rsvc_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(et_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))\n",
    "\n",
    "print(rsvc_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"RSVC Train: {}\\nRSVC Test: {}\\n----\".format(rsvc_grid.score(X_train, y_train), rsvc_grid.score(X_test, y_test)))\n",
    "\n",
    "print(adb_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a slight improvement going to a standard scaler with TF-IDF vectorizing for Extra Trees but we see that Adaptive Boosting's performance has dropped actually and Radial SVC has dropped dramatically as well. For Radial SVC it seems likely that the standard scaler is the issue so we can retry without that and for Adaptive Boosting let's try both TF-IDF and count vectorizer without a scaler before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer()\n",
      "RSVC Train: 0.9338588496120211\n",
      "RSVC Test: 0.902844477281123\n",
      "----\n",
      "TfidfVectorizer()\n",
      "AdaBoost Train: 0.9022047050129326\n",
      "AdaBoost Test: 0.8766161802733653\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "text_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer()) # No SS\n",
    "    ]\n",
    ")\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "    [\n",
    "        ('meta', meta_pipe, make_column_selector(dtype_include = np.number)),\n",
    "        ('text', text_pipe, 'response_cleaned'),\n",
    "        ('ohe', subreddit_pipe, ['subreddit'])\n",
    "    ],\n",
    "    n_jobs = 6\n",
    ")\n",
    "\n",
    "rsvc_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', col_trans),\n",
    "        ('rsvc', SVC(kernel = 'rbf'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "adbc_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', col_trans),\n",
    "        ('adbc', AdaBoostClassifier())\n",
    "    ]\n",
    ")\n",
    "\n",
    "vect_params = {\n",
    "    'ct__text__vect' : [TfidfVectorizer()]\n",
    "}\n",
    "\n",
    "rsvc_grid = GridSearchCV(\n",
    "    rsvc_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(rsvc_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"RSVC Train: {}\\nRSVC Test: {}\\n----\".format(rsvc_grid.score(X_train, y_train), rsvc_grid.score(X_test, y_test)))\n",
    "\n",
    "print(adb_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see both models prefer the Tf-IDF vectorizer over the count vectorizer. Despite the slightly lower test performance for Adaptive Boosting let's trust that cross-validation is a better approximate for our model's true performance and proceed with TF-IDF vectorizer. Our slightly increased overfit should give us some pause but let's set that aside for now. We also see that the standard scaler had no effect for Adaptive Boosting which is interesting. Radial SVC does have a notable performance bump bringing it barely to first place at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_pipe = Pipeline(\n",
    "    [\n",
    "        ('ss', StandardScaler()) # No need to worry about negative values anymore\n",
    "    ]\n",
    ")\n",
    "\n",
    "et_text_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('ss', StandardScaler(with_mean = False)) # It helps 2 models and doesn't affect the third so we can re-use the same pipe for all 3\n",
    "    ]\n",
    ")\n",
    "\n",
    "text_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', TfidfVectorizer())\n",
    "    ]\n",
    ")\n",
    "\n",
    "subreddit_pipe = Pipeline(\n",
    "    [\n",
    "        ('ohe', OneHotEncoder())\n",
    "    ]\n",
    ")\n",
    "\n",
    "et_col_trans = ColumnTransformer(\n",
    "    [\n",
    "        ('meta', meta_pipe, make_column_selector(dtype_include = np.number)),\n",
    "        ('text', et_text_pipe, 'response_cleaned'),\n",
    "        ('ohe', subreddit_pipe, ['subreddit'])\n",
    "    ],\n",
    "    n_jobs = 6\n",
    ")\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "    [\n",
    "        ('meta', meta_pipe, make_column_selector(dtype_include = np.number)),\n",
    "        ('text', text_pipe, 'response_cleaned'),\n",
    "        ('ohe', subreddit_pipe, ['subreddit'])\n",
    "    ],\n",
    "    n_jobs = 6\n",
    ")\n",
    "\n",
    "et_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', et_col_trans),\n",
    "        ('et', ExtraTreesClassifier(random_state = rs, n_jobs = 6))\n",
    "    ]\n",
    ")\n",
    "\n",
    "rsvc_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', col_trans),\n",
    "        ('rsvc', SVC(kernel = 'rbf'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "adbc_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', col_trans),\n",
    "        ('adbc', AdaBoostClassifier())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(ngram_range=(1, 2))\n",
      "ET Train: 1.0\n",
      "ET Test: 0.9050609530845954\n",
      "----\n",
      "TfidfVectorizer()\n",
      "RSVC Train: 0.9338588496120211\n",
      "RSVC Test: 0.902844477281123\n",
      "----\n",
      "TfidfVectorizer(ngram_range=(1, 2))\n",
      "AdaBoost Train: 0.9033132159132898\n",
      "AdaBoost Test: 0.8832656076837828\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "vect_params = {\n",
    "    'ct__text__vect__ngram_range' : ((1,1), (1,2), (1,3)),\n",
    "    'ct__text__vect__stop_words' : (None, 'english', stopwords.words('english'))\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "rsvc_grid = GridSearchCV(\n",
    "    rsvc_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(et_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))\n",
    "\n",
    "print(rsvc_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"RSVC Train: {}\\nRSVC Test: {}\\n----\".format(rsvc_grid.score(X_train, y_train), rsvc_grid.score(X_test, y_test)))\n",
    "\n",
    "print(adb_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All models prefer having the stopwords included which was expected, we also see that RSVC prefers 1-grams but Extra Trees and Adaptive Boosting prefers bigrams included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(max_features=3000, ngram_range=(1, 2))\n",
      "ET Train: 1.0\n",
      "ET Test: 0.9298115995567049\n",
      "----\n",
      "TfidfVectorizer(max_features=1000, min_df=100)\n",
      "RSVC Train: 0.9358295356571006\n",
      "RSVC Test: 0.9142962689323975\n",
      "----\n",
      "TfidfVectorizer(max_features=2000, ngram_range=(1, 2))\n",
      "AdaBoost Train: 0.906269244980909\n",
      "AdaBoost Test: 0.8810491318803103\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "vect_params = {\n",
    "    'ct__text__vect__ngram_range' : [(1,2)],\n",
    "    'ct__text__vect__max_features' : (1000, 2000, 3000),\n",
    "    'ct__text__vect__max_df' : (1.0, 0.9, 0.8),\n",
    "    'ct__text__vect__min_df' : (1, 10, 100)\n",
    "}\n",
    "\n",
    "rsvc_vect_params = {\n",
    "    'ct__text__vect__max_features' : (1000, 2000, 3000),\n",
    "    'ct__text__vect__max_df' : (1.0, 0.9, 0.8),\n",
    "    'ct__text__vect__min_df' : (1, 10, 100)\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5, \n",
    "    error_score = 'raise'\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "rsvc_grid = GridSearchCV(\n",
    "    rsvc_pipe,\n",
    "    param_grid = rsvc_vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(et_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))\n",
    "\n",
    "print(rsvc_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"RSVC Train: {}\\nRSVC Test: {}\\n----\".format(rsvc_grid.score(X_train, y_train), rsvc_grid.score(X_test, y_test)))\n",
    "\n",
    "print(adb_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each model has its own preferences for hyperparameters so we'll need to tune them separately moving forward. Let's start with Extra Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning TF-IDF Vectorizer for Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(max_df=0.5, max_features=3000, ngram_range=(1, 2))\n",
      "ET Train: 1.0\n",
      "ET Test: 0.9264868858514961\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "et_vect_params = {\n",
    "    'ct__text__vect__ngram_range' : [(1,2)],\n",
    "    'ct__text__vect__max_features' : (2000, 2500, 3000, 4000, 5000),\n",
    "    'ct__text__vect__max_df' : (1.0, 0.5),\n",
    "    'ct__text__vect__min_df' : (1, 0.1, 0.2)\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = et_vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(et_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This best model has very slightly worse test performance but it also has a very extreme value for max_df so let's try and narrow down the true ideal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(max_df=0.7, max_features=2750, ngram_range=(1, 2))\n",
      "ET Train: 1.0\n",
      "ET Test: 0.9301810121906169\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "et_vect_params = {\n",
    "    'ct__text__vect__ngram_range' : [(1,2)],\n",
    "    'ct__text__vect__max_features' : (2750, 3000, 3250, 3500),\n",
    "    'ct__text__vect__max_df' : (1.0, 0.75, 0.7, 0.6)\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = et_vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(et_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(max_df=0.75, max_features=2750, ngram_range=(1, 2))\n",
      "ET Train: 1.0\n",
      "ET Test: 0.925748060583672\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "et_vect_params = {\n",
    "    'ct__text__vect__ngram_range' : [(1,2)],\n",
    "    'ct__text__vect__max_features' : [(2750)],\n",
    "    'ct__text__vect__max_df' : [(0.75)],\n",
    "    'ct__text__vect__min_df' : (1, 10, 100, 0.15)\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = et_vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(et_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks as close to optimal as we can get so let's set aside Extra Trees and move on to Radial SVC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning TF-IDF Vectorizer for Radial SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(max_features=1000, min_df=100)\n",
      "RSVC Train: 0.9358295356571006\n",
      "RSVC Test: 0.9142962689323975\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "vect_params = {\n",
    "    'ct__text__vect__max_features' : (1000, 500, 1500, 750),\n",
    "    'ct__text__vect__min_df' : (100, 50, 75, 150, 200)\n",
    "}\n",
    "\n",
    "rsvc_grid = GridSearchCV(\n",
    "    rsvc_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(rsvc_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"RSVC Train: {}\\nRSVC Test: {}\\n----\".format(rsvc_grid.score(X_train, y_train), rsvc_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(max_features=800, min_df=100)\n",
      "RSVC Train: 0.9358295356571006\n",
      "RSVC Test: 0.9142962689323975\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "vect_params = {\n",
    "    'ct__text__vect__max_features' : (800, 900, 1000, 1100, 1250),\n",
    "    'ct__text__vect__min_df' : (100, 500)\n",
    "}\n",
    "\n",
    "rsvc_grid = GridSearchCV(\n",
    "    rsvc_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(rsvc_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"RSVC Train: {}\\nRSVC Test: {}\\n----\".format(rsvc_grid.score(X_train, y_train), rsvc_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems like a good stopping point for RSVC let's move on to Adaptive Boosting before getting into tuning the hyperparameters of our model.s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning TF-IDF Vectorizer for Adaptive Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(max_features=2000, ngram_range=(1, 2))\n",
      "AdaBoost Train: 0.906269244980909\n",
      "AdaBoost Test: 0.8810491318803103\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "vect_params = {\n",
    "    'ct__text__vect__ngram_range' : [(1,2)],\n",
    "    'ct__text__vect__max_features' : (1750, 2000, 2250, 2500),\n",
    "    'ct__text__vect__max_df' : (1.0, 0.7, 0.6, 0.5),\n",
    "    'ct__text__vect__min_df' : (1, 10, 100)\n",
    "}\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(adb_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(max_features=2000, ngram_range=(1, 2))\n",
      "AdaBoost Train: 0.906269244980909\n",
      "AdaBoost Test: 0.8810491318803103\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "vect_params = {\n",
    "    'ct__text__vect__ngram_range' : [(1,2)],\n",
    "    'ct__text__vect__max_features' : [(2000)],\n",
    "    'ct__text__vect__min_df' : (1, 0.1, 0.2, 0.05)\n",
    "}\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(adb_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model seems mostly happy with this combination but this is relatively high overfitting compared to how this model was doing earlier so let's try ignoring this set of max_features and trying to find a comparable one with less overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(max_features=1250, ngram_range=(1, 2))\n",
      "AdaBoost Train: 0.9004803547234881\n",
      "AdaBoost Test: 0.8865903213889915\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "vect_params = {\n",
    "    'ct__text__vect__ngram_range' : [(1,2)],\n",
    "    'ct__text__vect__max_features' : (500, 600, 700, 800, 900, 1000, 1250)\n",
    "}\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(adb_grid.best_estimator_['ct'].transformers[1][1]['vect'])\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This actually seems to have worked, slightly less train accuracy but slightly higher test accuracy and less overfit even if only slightly. Moreover this model uses less features which is preferable where possible for time, memory, and variance reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our Count Vectorizer parameters, let's redefine our pipes with the new hyper parameters and get ready to tune hyperparameters for our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_pipe = Pipeline(\n",
    "    [\n",
    "        ('ss', StandardScaler()) # No need to worry about negative values anymore\n",
    "    ]\n",
    ")\n",
    "\n",
    "et_text_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', TfidfVectorizer(max_df = 0.75, max_features = 2750, ngram_range = (1, 2))),\n",
    "        ('ss', StandardScaler(with_mean = False)) # It helps 2 models and doesn't affect the third so we can re-use the same pipe for all 3\n",
    "    ]\n",
    ")\n",
    "\n",
    "rsvc_text_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', TfidfVectorizer(max_features = 800, min_df = 100))\n",
    "    ]\n",
    ")\n",
    "\n",
    "adb_text_pipe = Pipeline(\n",
    "    [\n",
    "        ('vect', TfidfVectorizer(max_features = 1250, ngram_range = (1, 2)))\n",
    "    ]\n",
    ")\n",
    "\n",
    "subreddit_pipe = Pipeline(\n",
    "    [\n",
    "        ('ohe', OneHotEncoder())\n",
    "    ]\n",
    ")\n",
    "\n",
    "et_col_trans = ColumnTransformer(\n",
    "    [\n",
    "        ('meta', meta_pipe, make_column_selector(dtype_include = np.number)),\n",
    "        ('text', et_text_pipe, 'response_cleaned'),\n",
    "        ('ohe', subreddit_pipe, ['subreddit'])\n",
    "    ],\n",
    "    n_jobs = 6\n",
    ")\n",
    "\n",
    "rsvc_col_trans = ColumnTransformer(\n",
    "    [\n",
    "        ('meta', meta_pipe, make_column_selector(dtype_include = np.number)),\n",
    "        ('text', rsvc_text_pipe, 'response_cleaned'),\n",
    "        ('ohe', subreddit_pipe, ['subreddit'])\n",
    "    ],\n",
    "    n_jobs = 6\n",
    ")\n",
    "\n",
    "adb_col_trans = ColumnTransformer(\n",
    "    [\n",
    "        ('meta', meta_pipe, make_column_selector(dtype_include = np.number)),\n",
    "        ('text', adb_text_pipe, 'response_cleaned'),\n",
    "        ('ohe', subreddit_pipe, ['subreddit'])\n",
    "    ],\n",
    "    n_jobs = 6\n",
    ")\n",
    "\n",
    "et_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', et_col_trans),\n",
    "        ('et', ExtraTreesClassifier(random_state = rs, n_jobs = 6))\n",
    "    ]\n",
    ")\n",
    "\n",
    "rsvc_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', rsvc_col_trans),\n",
    "        ('rsvc', SVC(kernel = 'rbf'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "adbc_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', adb_col_trans),\n",
    "        ('adbc', AdaBoostClassifier())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning hyperparameters for RSVC\n",
    "\n",
    "Let's start with our SVC model because it has the fewest hyperparameters and the worst performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=5)\n",
      "RSVC Train: 0.9798004680379357\n",
      "RSVC Test: 0.9283339490210565\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Used https://www.vebuso.com/2020/03/svm-hyperparameter-tuning-using-gridsearchcv/ along with the documentation to understand these parameters\n",
    "rsvc_params = {\n",
    "    'rsvc__C' : (1.0, 0.1, 10, 0.5, 5), # This parameter is functionally the same as it is for Ridge regression since this is the l2 regularization coefficient\n",
    "    'rsvc__gamma' : ('scale', 'auto', 1.0, 0.1, 0.01, 0.001) # This parameter controls how far away the support vectors we consider can be (lower allows for further vectors)\n",
    "}\n",
    "\n",
    "rsvc_grid = GridSearchCV(\n",
    "    rsvc_pipe,\n",
    "    param_grid = rsvc_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(rsvc_grid.best_estimator_['rsvc'])\n",
    "print(\"RSVC Train: {}\\nRSVC Test: {}\\n----\".format(rsvc_grid.score(X_train, y_train), rsvc_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization has improved this model by quite a bit and we see that the default gamma is currently preferred. The default setting of 'scale' uses $\\frac{1}{n_features * X.var()}$ which means that its picking an appropriate and already specific value that is scaled to this model. Let's continue this approach a bit longer and see if we get anything noticably better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=5, gamma=0.2)\n",
      "RSVC Train: 0.9938416061091268\n",
      "RSVC Test: 0.930550424824529\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "rsvc_params = {\n",
    "    'rsvc__C' : (2.5, 3, 5, 6, 7.5, 9), \n",
    "    'rsvc__gamma' : (0.1, 0.2, 0.5, 0.05) \n",
    "}\n",
    "\n",
    "rsvc_grid = GridSearchCV(\n",
    "    rsvc_pipe,\n",
    "    param_grid = rsvc_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(rsvc_grid.best_estimator_['rsvc'])\n",
    "print(\"RSVC Train: {}\\nRSVC Test: {}\\n----\".format(rsvc_grid.score(X_train, y_train), rsvc_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this model does perform at the slightly better than our previous one, it seems that changing gamma has actually led to a harsher overfit, instead let's stop tuning gamma and use the default while we tune only C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=8)\n",
      "RSVC Train: 0.9884222194851583\n",
      "RSVC Test: 0.9272257111193203\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "rsvc_params = {\n",
    "    'rsvc__C' : (4, 4.5, 5, 5.5, 8)\n",
    "}\n",
    "\n",
    "rsvc_grid = GridSearchCV(\n",
    "    rsvc_pipe,\n",
    "    param_grid = rsvc_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(rsvc_grid.best_estimator_['rsvc'])\n",
    "print(\"RSVC Train: {}\\nRSVC Test: {}\\n----\".format(rsvc_grid.score(X_train, y_train), rsvc_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we see a bit of the overfit issue here but its actually less than we had before so let's try a bit of gamma tuning at this level of C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=8)\n",
      "RSVC Train: 0.9884222194851583\n",
      "RSVC Test: 0.9272257111193203\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "rsvc_params = {\n",
    "    'rsvc__C' : [(8)],\n",
    "    'rsvc__gamma' : ('scale', 'auto', 0.1, 0.5, 1.0, 2.0, 5.0, 50.0)\n",
    "}\n",
    "\n",
    "rsvc_grid = GridSearchCV(\n",
    "    rsvc_pipe,\n",
    "    param_grid = rsvc_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(rsvc_grid.best_estimator_['rsvc'])\n",
    "print(\"RSVC Train: {}\\nRSVC Test: {}\\n----\".format(rsvc_grid.score(X_train, y_train), rsvc_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems like a good stopping point. We could find a slightly better value of C and we have the overfit issue still but this is much better than where we started and not the most overfit we've had so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning hyper parameters for Adaptive Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(n_estimators=200)\n",
      "AdaBoost Train: 0.9497475058504742\n",
      "AdaBoost Test: 0.9080162541558922\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "adb_params = {\n",
    "    'adbc__n_estimators' : (50, 25, 75, 100, 150, 200), # The number of rounds of boosting, more estimators means more simple models trained in sequence\n",
    "    'adbc__learning_rate' : (1.0, 2.0, 5.0, 10.0, 50.0) # Changes the weight applied to each estimator, a higher rate means each classifier is individually a greater vote\n",
    "}\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = adb_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(adb_grid.best_estimator_['adbc'])\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see our model prefers the original learning rate but let's investigate that further before we move on to further tuning the number of estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(n_estimators=200)\n",
      "AdaBoost Train: 0.9497475058504742\n",
      "AdaBoost Test: 0.9080162541558922\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "adb_params = {\n",
    "    'adbc__n_estimators' : (50, 25, 75, 100, 150, 200), \n",
    "    'adbc__learning_rate' : (1.0, 1.1, 1.5, 1.75) \n",
    "}\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = adb_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(adb_grid.best_estimator_['adbc'])\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like slight changes on learning rate are still not changing much so let's set it aside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(n_estimators=400)\n",
      "AdaBoost Train: 0.980416307427023\n",
      "AdaBoost Test: 0.9069080162541558\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "adb_params = {\n",
    "    'adbc__n_estimators' : (250, 300, 400, 500, 1000)\n",
    "}\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = adb_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(adb_grid.best_estimator_['adbc'])\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(n_estimators=400)\n",
      "AdaBoost Train: 0.980416307427023\n",
      "AdaBoost Test: 0.9069080162541558\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "adb_params = {\n",
    "    'adbc__n_estimators' : (400, 450, 350, 300)\n",
    "}\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = adb_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(adb_grid.best_estimator_['adbc'])\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not really getting anywhere here so let's step back and adjust learning rate to see if we can find any improvements there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(n_estimators=400)\n",
      "AdaBoost Train: 0.980416307427023\n",
      "AdaBoost Test: 0.9069080162541558\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "adb_params = {\n",
    "    'adbc__n_estimators' : [(400)],\n",
    "    'adbc__learning_rate' : (1.0, 5, 2.5, 1.1, 100, 500)\n",
    "}\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = adb_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(adb_grid.best_estimator_['adbc'])\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we can say that this line of reasoning simply isn't working. Let's try some more values for number of estimators because its currently lagging behind our other model's performance. We could be at a soft maximum for performance and be missing a sizable performance gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(n_estimators=400)\n",
      "AdaBoost Train: 0.980416307427023\n",
      "AdaBoost Test: 0.9069080162541558\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "adb_params = {\n",
    "    'adbc__n_estimators' : (75, 150, 400, 1250, 1500, 1750, 2500, 5000),\n",
    "}\n",
    "\n",
    "adb_grid = GridSearchCV(\n",
    "    adbc_pipe,\n",
    "    param_grid = adb_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(adb_grid.best_estimator_['adbc'])\n",
    "print(\"AdaBoost Train: {}\\nAdaBoost Test: {}\\n----\".format(adb_grid.score(X_train, y_train), adb_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We spent a bit longer tuning just to get a better feel for how the number of estimators and learning rate change our accuracy over time. We hadn't covered this in particular in class so spending the extra time to build that familiarity seemed warranted. One thing that stands out her is that while our test score went up, so did the gap between our training and test splits. This is notable but not surprising since more estimators intuitively should lead to more overfit. Since our perfomance went up a fair bit and these models are doing better through cross-validation we can comfortably accept these are better models. We will proceed with the 200 estimator model solely for the reason that it performed about the same with less gap between training and test split. Let's move on to Extra Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning hyper parameters for Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(n_estimators=200, n_jobs=6, random_state=91923)\n",
      "ET Train: 1.0\n",
      "ET Test: 0.9287033616549686\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# There are a LOT of parameters for Extra Trees so tuning many values at once is difficult\n",
    "et_vect_params = {\n",
    "    'et__n_estimators' : (50, 100, 200), # Number of trees\n",
    "    'et__max_depth' : (None, 5, 10), # How deep each tree can be (higher is better at predicting but can lead to overfit)\n",
    "    'et__bootstrap' : (False, True), # Whether or not each tree is trained on bootstrapped data or the original data\n",
    "    'et__max_features' : (None, 'sqrt', 'log2'), # The maximum number of features to use in a given tree\n",
    "    'et__min_samples_split' : (2, 10, 50) # The minimum number of elements in a leaf node\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = et_vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(et_grid.best_estimator_['et'])\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best model seems to be using the highest number of estimators so let's keep increasing there and try a few caps on maximum depth while we're at it to try and combat overfitting. The test accurary is actually a bit lower than the default so we should keep in mind that the default is at least comparable to this model despite being simpler with fewer estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(n_estimators=500, n_jobs=6, random_state=91923)\n",
      "ET Train: 1.0\n",
      "ET Test: 0.9301810121906169\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "et_vect_params = {\n",
    "    'et__n_estimators' : (150, 200, 300, 500),\n",
    "    'et__max_depth' : (None, 15, 50)\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = et_vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(et_grid.best_estimator_['et'])\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we see the highest number of estimators is best and the model does not like having a maximum depth. Let's try more estimators again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(n_estimators=750, n_jobs=6, random_state=91923)\n",
      "ET Train: 1.0\n",
      "ET Test: 0.9290727742888807\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "et_vect_params = {\n",
    "    'et__n_estimators' : (450, 500, 750, 1000),\n",
    "    'et__max_depth' : (None, 15, 25)\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = et_vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(et_grid.best_estimator_['et'])\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've honed in a bit on a prefered number of estimators so let's try the other hyper parameters some more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(max_depth=50, min_samples_split=4, n_estimators=900,\n",
      "                     n_jobs=6, random_state=91923)\n",
      "ET Train: 0.9956891242763887\n",
      "ET Test: 0.9194680458071666\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "et_vect_params = {\n",
    "    'et__n_estimators' : (600, 700, 750, 800, 900),\n",
    "    'et__max_depth' : (5, 15, 25, 50),\n",
    "    'et__min_samples_split' : (2, 4, 6),\n",
    "    'et__min_samples_leaf' : (1, 2, 3)\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = et_vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(et_grid.best_estimator_['et'])\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't seem like this experiment has yielded any good results so let's make one more attempt before we move on. This time we'll limit max depth again but provided more estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(max_depth=75, n_estimators=1250, n_jobs=6,\n",
      "                     random_state=91923)\n",
      "ET Train: 0.9995073284887301\n",
      "ET Test: 0.9287033616549686\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "et_vect_params = {\n",
    "    'et__n_estimators' : (900, 1000, 1250, 1500),\n",
    "    'et__max_depth' : (40, 50, 60, 75)\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = et_vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(et_grid.best_estimator_['et'])\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an interesting result and does bear some further consideration. Since it seems like we can get comparable results this way let's try and improve this further before going back to our best performing model overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(max_depth=100, min_samples_leaf=2, min_samples_split=8,\n",
      "                     n_estimators=1250, n_jobs=6, random_state=91923)\n",
      "ET Train: 0.9954427885207537\n",
      "ET Test: 0.9253786479497599\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "et_vect_params = {\n",
    "    'et__n_estimators' : [(1250)],\n",
    "    'et__max_depth' : (75, 80, 100),\n",
    "    'et__min_samples_split' : (4, 8, 12),\n",
    "    'et__min_samples_leaf' : (2, 5, 7)\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe,\n",
    "    param_grid = et_vect_params,\n",
    "    n_jobs = 6,\n",
    "    cv = 5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(et_grid.best_estimator_['et'])\n",
    "print(\"ET Train: {}\\nET Test: {}\\n----\".format(et_grid.score(X_train, y_train), et_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point its become evident that this approach is unlikely to show better performance that our earlier model with fewer non-default parameters. Between the 500 and the 750 estimator models we will err on the side that fewer estimators is better within the same tier of performance since they have near identical accuracies. This is in part for the theoretical benefit of avoiding overfit but mainly we want the simpler model that will run a bit faster if only marginally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Models and Pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', et_col_trans),\n",
    "        ('et', ExtraTreesClassifier(n_estimators = 500, random_state = rs, n_jobs = 6))\n",
    "    ]\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "rsvc_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', rsvc_col_trans),\n",
    "        ('rsvc', SVC(kernel = 'rbf', C = 8))\n",
    "    ]\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "adbc_pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', adb_col_trans),\n",
    "        ('adbc', AdaBoostClassifier(n_estimators = 200))\n",
    "    ]\n",
    ").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pickle_jar/ExtraTrees.pkl', 'wb') as f:\n",
    "    pickle.dump(et_pipe, f)\n",
    "\n",
    "with open('./pickle_jar/RSVC.pkl', 'wb') as f:\n",
    "    pickle.dump(rsvc_pipe, f)\n",
    "\n",
    "with open('./pickle_jar/AdaBoost.pkl', 'wb') as f:\n",
    "    pickle.dump(adbc_pipe, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
